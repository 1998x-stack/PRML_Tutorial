# 00_7.2.1_RVM_for_regression

"""
Lecture: 7_Sparse_Kernel_Machines/7.2_Relevance_Vector_Machines
Content: 00_7.2.1_RVM_for_regression
"""

### 相关向量机用于回归 (RVM for Regression) 详细分析

#### 1. 引言
相关向量机（Relevance Vector Machine，RVM）是一种贝叶斯稀疏核技术，用于回归和分类任务。与支持向量机（SVM）类似，RVM 旨在构建稀疏模型，但它采用贝叶斯框架，使得模型具有概率解释。相比于SVM，RVM模型通常更加稀疏，因此在测试数据上的性能更快，同时保持了可比的泛化误差。

#### 2. 模型定义
RVM是一种线性模型，其形式与第3章中研究的模型相似，但其先验经过修改以产生稀疏解。模型定义了一个给定输入向量 $x$ 的实值目标变量 $t$ 的条件分布，其形式为：
$$ p(t|x,w, \beta) = N (t|y(x), \beta^{-1}) $$
其中，$\beta = \sigma^{-2}$ 是噪声精度（噪声方差的倒数），均值由以下线性模型给出：
$$ y(x) = \sum_{i=1}^{M} w_i \phi_i(x) = w^T \phi(x) $$
其中 $\phi_i(x)$ 是固定的非线性基函数，通常包括一个常数项，以便相应的权重参数表示偏置。

#### 3. 核函数与稀疏性
RVM模型的基础函数由核函数给出，每个核函数与训练集中的一个数据点相关联。通用表达式可以写成类似SVM的形式：
$$ y(x) = \sum_{n=1}^{N} w_n k(x,x_n) + b $$
其中，$b$ 是偏置参数。在这种情况下，参数的数量为 $M = N + 1$，$y(x)$ 的形式与SVM的预测模型相同，只是系数 $a_n$ 在这里表示为 $w_n$。

#### 4. 先验分布与后验分布
RVM引入了一个对参数向量 $w$ 的先验分布。与第3章中类似，我们考虑一个零均值高斯先验，但RVM的关键区别在于我们为每个权重参数 $w_i$ 引入了一个独立的超参数 $\alpha_i$，而不是单一的共享超参数。因此，权重的先验分布形式为：
$$ p(w|\alpha) = \prod_{i=1}^{M} N (w_i|0, \alpha_i^{-1}) $$
其中，$\alpha_i$ 表示相应参数 $w_i$ 的精度，$\alpha$ 表示所有超参数的向量。

通过最大化证据的方法（也称为类型2最大似然法），我们可以确定 $\alpha$ 和 $\beta$ 的值，从而得到稀疏模型。

#### 5. 证据最大化与优化
证据最大化的边缘似然函数通过对权重参数进行积分得到：
$$ p(t|X, \alpha, \beta) = \int p(t|X,w, \beta)p(w|\alpha) dw $$
后验分布依然是高斯分布，形式为：
$$ p(w|t,X, \alpha, \beta) = N (w|m, \Sigma) $$
其中，均值和协方差为：
$$ m = \beta \Sigma \Phi^T t $$
$$ \Sigma = (A + \beta \Phi^T \Phi)^{-1} $$
其中，$\Phi$ 是设计矩阵，元素 $\Phi_{ni} = \phi_i(x_n)$，$A = \text{diag}(\alpha_i)$。

#### 6. 稀疏性与相关向量
在最大化证据的过程中，一部分超参数 $\alpha_i$ 会趋于无穷大，相应的权重参数 $w_i$ 的后验分布会集中在零。这样，与这些参数相关的基函数在模型中不起作用，从而实现稀疏性。剩余的非零权重对应的输入点被称为相关向量（Relevance Vectors）。

#### 7. 预测分布
给定新的输入 $x$，可以评估目标变量 $t$ 的预测分布：
$$ p(t|x,X, t, \alpha, \beta) = N (t|m^T \phi(x), \sigma^2(x)) $$
其中，预测均值为 $m^T \phi(x)$，预测方差为：
$$ \sigma^2(x) = \beta^{-1} + \phi(x)^T \Sigma \phi(x) $$

#### 8. 计算成本与优势
RVM的主要优势在于其稀疏性，通常可以生成比SVM更加紧凑的模型，从而在测试数据上的处理速度更快。然而，RVM的训练涉及优化一个非凸函数，训练时间可能比SVM更长。对于具有 $M$ 个基函数的模型，RVM需要求解一个大小为 $M \times M$ 的矩阵的逆，计算成本为 $O(M^3)$。

#### 9. 实际应用
在实际应用中，RVM被发现能够在广泛的回归和分类任务中生成比SVM更加稀疏的模型，从而显著提高了测试数据上的处理速度。尽管RVM的训练时间较长，但其在测试阶段的高效性和稀疏性使其在许多应用中具有吸引力。

### 总结
相关向量机通过引入独立的超参数和贝叶斯框架，实现了对回归问题的稀疏解。与支持向量机相比，RVM在保持可比泛化误差的同时，通常能生成更加稀疏的模型，从而在测试数据上表现出更高的处理速度。尽管其训练时间较长，但其稀疏性和高效性使其在许多实际应用中具有重要意义。

### 参考文献
- Tipping, M. E. (2001). Sparse Bayesian Learning and the Relevance Vector Machine. Journal of Machine Learning Research, 1, 211-244.
- Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.