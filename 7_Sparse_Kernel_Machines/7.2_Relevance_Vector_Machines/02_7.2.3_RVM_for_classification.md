# 02_7.2.3_RVM_for_classification

"""
Lecture: 7_Sparse_Kernel_Machines/7.2_Relevance_Vector_Machines
Content: 02_7.2.3_RVM_for_classification
"""

### 7.2.3 分类中的相关向量机（RVM）

#### 概述
在《模式识别与机器学习》中，7.2.3节详细介绍了如何将相关向量机（Relevance Vector Machine, RVM）框架扩展到分类问题中。与回归模型不同，分类问题中使用了概率线性分类模型，并通过自动相关性确定（Automatic Relevance Determination, ARD）先验来实现稀疏性。

#### 模型形式
对于二分类问题，目标变量 $ t \in \{0, 1\} $。模型形式为：

$$ y(x, w) = \sigma(w^T \phi(x)) $$

其中，$ \sigma(\cdot) $ 是逻辑Sigmoid函数，定义为：

$$ \sigma(a) = \frac{1}{1 + e^{-a}} $$

通过对权重向量 $ w $ 引入高斯先验，得到模型形式为：

$$ p(w | \alpha) = \mathcal{N}(w | 0, A^{-1}) $$

其中，$ A $ 是一个对角矩阵，其对角元素为各个权重参数的精度超参数 $ \alpha_i $。

#### 贝叶斯推断
与回归模型不同，分类模型中无法对参数向量 $ w $ 进行解析积分。因此，使用拉普拉斯近似法来处理，即用一个高斯分布来近似后验分布。这种方法在第4章中已经用于贝叶斯逻辑回归。

首先，初始化超参数向量 $ \alpha $。对于给定的 $ \alpha $ 值，构建后验分布的高斯近似，从而得到边缘似然的近似值。最大化该近似边缘似然可以重新估计 $ \alpha $ 值，重复该过程直到收敛。

#### 拉普拉斯近似
拉普拉斯近似的详细步骤如下：

1. **后验分布模式**：对于固定的 $ \alpha $ 值，通过最大化 $ \ln p(w|t, \alpha) $ 得到后验分布的模式，即：

$$ \ln p(w|t, \alpha) = \ln \{p(t|w)p(w|\alpha)\} - \ln p(t|\alpha) $$

2. **最大化对数后验分布**：可以使用迭代重加权最小二乘法（IRLS）进行最大化，对数后验分布的梯度向量和Hessian矩阵为：

$$ \nabla \ln p(w|t, \alpha) = \Phi^T(t - y) - Aw $$

$$ \nabla \nabla \ln p(w|t, \alpha) = -(\Phi^T B \Phi + A) $$

其中，$ B $ 是对角矩阵，其元素为 $ b_n = y_n (1 - y_n) $，$ y $ 是模型输出向量。

3. **近似边缘似然**：使用拉普拉斯近似法计算边缘似然：

$$ \ln p(t|\alpha) \approx \ln p(t|w_{MAP}) - \frac{1}{2} w_{MAP}^T A w_{MAP} - \frac{1}{2} \ln |H| + \text{const} $$

其中，$ w_{MAP} $ 为后验分布的模式，$ H $ 为后验分布的Hessian矩阵。

#### 多分类扩展
对于多分类问题，使用K个线性模型，通过softmax函数组合输出：

$$ y_k(x) = \frac{\exp(a_k)}{\sum_j \exp(a_j)} $$

其中，$ a_k = w_k^T x $。

对数似然函数为：

$$ \ln p(T|w_1, \ldots, w_K) = \sum_{n=1}^N \sum_{k=1}^K t_{nk} \ln y_{nk} $$

目标值 $ t_{nk} $ 使用K分类编码。使用拉普拉斯近似法优化超参数，这种方法比SVM中的成对分类方法更为原则性，并为新数据点提供概率预测。

#### 优缺点
RVM的主要优点在于稀疏性和概率预测。然而，训练时间相对较长。尽管如此，RVM通过自动确定模型复杂度参数，避免了交叉验证的需求，在处理测试数据时计算时间较短。训练时需要对基函数数量 $ M $ 进行矩阵求逆，计算复杂度为 $ O(M^3) $，这比SVM的训练时间要长。
