# 02_4.2.3_Discrete_features

"""
Lecture: 4_Linear_Models_for_Classification/4.2_Probabilistic_Generative_Models
Content: 02_4.2.3_Discrete_features
"""

## 详解PRML中的第4.2.3节：离散特征

《模式识别与机器学习》（Pattern Recognition and Machine Learning, PRML）是由Christopher M. Bishop所著的一本经典教材，其中第4章涵盖了线性分类模型的内容。在第4.2节，作者介绍了概率生成模型（Probabilistic Generative Models）。具体来说，第4.2.3节探讨了离散特征的处理方法。以下是对这一节内容的详细分析。

### 背景介绍

在许多实际应用中，输入数据的特征可能是离散的而非连续的。例如，文本数据中的词汇特征、基因序列中的碱基特征等都是离散的。在处理这些离散特征时，我们需要对概率生成模型进行相应的调整，以便能够有效地建模和分类。

### 离散特征的概率生成模型

对于离散特征 $x_i$，我们假设它们是二进制特征，即 $x_i \in \{0, 1\}$。对于每一个特征，我们希望能够建模其在不同类别下的条件概率分布。为了简化问题，我们通常假设特征在给定类别条件下是相互独立的（即朴素贝叶斯假设）。

#### 朴素贝叶斯假设

根据朴素贝叶斯假设，给定类别 $C_k$ 的条件下，特征 $x_i$ 的条件概率分布可以表示为：

$$ p(x|C_k) = \prod_{i=1}^{D} \mu_{ki}^{x_i} (1 - \mu_{ki})^{1-x_i} $$

其中，$\mu_{ki}$ 表示类别 $C_k$ 下特征 $x_i$ 为1的概率，$D$ 是特征的数量。这种假设极大地简化了模型的复杂性，因为我们只需要估计 $D$ 个参数，而不是 $2^D$ 个。

### 对数线性模型

在使用朴素贝叶斯假设建模离散特征后，我们可以得到对数线性模型。给定输入向量 $x$，类别 $C_k$ 的对数几率可以表示为：

$$ a_k(x) = \sum_{i=1}^{D} \{ x_i \ln \mu_{ki} + (1 - x_i) \ln (1 - \mu_{ki}) \} + \ln p(C_k) $$

其中，$\ln p(C_k)$ 是类别的先验概率。对于二分类问题，我们可以使用逻辑回归模型来表示：

$$ p(C_1|x) = \frac{1}{1 + \exp(-a(x))} $$

其中，

$$ a(x) = \sum_{i=1}^{D} x_i \ln \frac{\mu_{1i}}{\mu_{2i}} + \sum_{i=1}^{D} (1 - x_i) \ln \frac{1 - \mu_{1i}}{1 - \mu_{2i}} + \ln \frac{p(C_1)}{p(C_2)} $$

### 参数估计

为了估计模型参数 $\mu_{ki}$ 和 $p(C_k)$，我们可以使用最大似然估计。对于类别 $C_k$，参数 $\mu_{ki}$ 的最大似然估计为：

$$ \mu_{ki} = \frac{\sum_{n \in C_k} x_{ni}}{N_k} $$

其中，$N_k$ 是类别 $C_k$ 中的样本数量，$x_{ni}$ 表示第 $n$ 个样本的第 $i$ 个特征。

先验概率 $p(C_k)$ 可以通过类别样本的比例来估计：

$$ p(C_k) = \frac{N_k}{N} $$

其中，$N$ 是总样本数。

### 扩展到多类别和多状态特征

虽然上述讨论集中于二分类问题和二进制特征，但这些方法可以扩展到多类别和多状态特征。对于多类别问题，我们可以引入一个独立的二进制分类器集合，每个分类器用于区分一个类别与其他类别（one-vs-rest方法）。对于多状态特征，我们可以将每个特征的不同状态编码为多个二进制特征，例如使用独热编码（one-hot encoding）。

### 应用场景和优势

离散特征的概率生成模型在文本分类、基因数据分析、推荐系统等领域有广泛应用。其优势在于：
- **计算简单**：由于朴素贝叶斯假设，模型训练和推断的计算复杂度较低。
- **鲁棒性好**：在高维稀疏数据中表现良好。
- **解释性强**：模型参数具有明确的概率意义，便于解释。

然而，朴素贝叶斯假设可能过于简化，忽略了特征之间的相互依赖性。在实际应用中，可以结合其他方法（如集成学习、特征选择）来提升模型性能。
