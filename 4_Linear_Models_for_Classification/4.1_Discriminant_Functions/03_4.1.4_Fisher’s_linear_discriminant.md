# 03_4.1.4_Fisher’s_linear_discriminant

"""
Lecture: 4_Linear_Models_for_Classification/4.1_Discriminant_Functions
Content: 03_4.1.4_Fisher’s_linear_discriminant
"""

### 4.1.4 Fisher线性判别分析

在《模式识别与机器学习》（PRML）一书的第4章中，Bishop博士详细介绍了线性分类模型的概念。第4.1节专注于判别函数，并在4.1.4节中讨论了Fisher线性判别分析（LDA）。以下是对4.1.4节内容的详细分析。

#### Fisher线性判别分析的基本概念

Fisher线性判别分析是一种将多维数据投影到一维空间的方法，以最大化类间的分离度并最小化类内的分散度。这种方法不仅用于分类任务，还用于降维任务。

考虑一个两类分类问题，其中有 $ N_1 $ 个点属于类 $ C_1 $，有 $ N_2 $ 个点属于类 $ C_2 $。这两类的均值向量分别为：

$$ \mathbf{m}_1 = \frac{1}{N_1} \sum_{n \in C_1} \mathbf{x}_n $$
$$ \mathbf{m}_2 = \frac{1}{N_2} \sum_{n \in C_2} \mathbf{x}_n $$

#### Fisher准则

Fisher提出了一种选择投影方向 $ \mathbf{w} $ 的准则，即最大化类间方差与类内方差之比。这一准则称为Fisher准则，其表达式为：

$$ J(\mathbf{w}) = \frac{\mathbf{w}^T \mathbf{S}_B \mathbf{w}}{\mathbf{w}^T \mathbf{S}_W \mathbf{w}} $$

其中，$ \mathbf{S}_B $ 是类间散布矩阵，定义为：

$$ \mathbf{S}_B = (\mathbf{m}_2 - \mathbf{m}_1)(\mathbf{m}_2 - \mathbf{m}_1)^T $$

$ \mathbf{S}_W $ 是类内散布矩阵，定义为：

$$ \mathbf{S}_W = \sum_{n \in C_1} (\mathbf{x}_n - \mathbf{m}_1)(\mathbf{x}_n - \mathbf{m}_1)^T + \sum_{n \in C_2} (\mathbf{x}_n - \mathbf{m}_2)(\mathbf{x}_n - \mathbf{m}_2)^T $$

通过对 $ J(\mathbf{w}) $ 求导并设其为0，可以得到最优投影方向 $ \mathbf{w} $ 的解析解：

$$ \mathbf{w} \propto \mathbf{S}_W^{-1} (\mathbf{m}_2 - \mathbf{m}_1) $$

#### Fisher判别函数

虽然Fisher线性判别不是严格意义上的判别函数，而是用于将数据投影到一维空间的方向选择，但投影后的数据可以用于构建判别函数。投影后的数据 $ y $ 可以通过如下公式计算：

$$ y = \mathbf{w}^T \mathbf{x} $$

根据投影后的值，可以选择阈值 $ y_0 $，以便分类新数据点。如果 $ y(\mathbf{x}) \geq y_0 $，则将点 $ \mathbf{x} $ 分配到类 $ C_1 $；否则分配到类 $ C_2 $。

例如，可以使用高斯分布对类条件密度 $ p(y|C_k) $ 建模，然后通过最大似然法找到高斯分布的参数。根据投影类的高斯近似，可以找到最优阈值。

### 结论

在第4.1.4节中，Bishop博士详细阐述了Fisher线性判别分析。这种方法通过选择最佳投影方向来最大化类间分离度并最小化类内分散度。尽管Fisher线性判别分析在理论上有很强的吸引力，但在实际应用中需要结合其他方法进行优化。