# 00_4.1.1_Two_classes

"""
Lecture: 4_Linear_Models_for_Classification/4.1_Discriminant_Functions
Content: 00_4.1.1_Two_classes
"""

### 4.1.1 两类分类问题

在《模式识别与机器学习》（PRML）一书的第4章中，Bishop博士详细介绍了线性分类模型的概念。第4.1节专注于判别函数，并在4.1.1节中讨论了两类分类问题。以下是对4.1.1节内容的详细分析。

#### 判别函数

判别函数是一种输入向量 $ \mathbf{x} $ 并将其分配到 $ K $ 类之一的函数，记为 $ C_k $。在本节中，我们主要讨论线性判别函数，即决策面是超平面（hyperplane）的情况。首先考虑两类分类问题，然后再探讨扩展到 $ K > 2 $ 类的情况。

#### 两类分类

线性判别函数的最简单表示是对输入向量进行线性函数处理，形式如下：

$$ y(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + w_0 $$

其中，$ \mathbf{w} $ 称为权重向量，$ w_0 $ 为偏置。将输入向量 $ \mathbf{x} $ 分配到类 $ C_1 $ 的条件是 $ y(\mathbf{x}) \geq 0 $，否则分配到类 $ C_2 $。对应的决策边界由 $ y(\mathbf{x}) = 0 $ 定义，即：

$$ \mathbf{w}^T \mathbf{x} + w_0 = 0 $$

这对应于 $ D $ 维输入空间中的一个 $ D-1 $ 维超平面。假设两个点 $ \mathbf{x}_A $ 和 $ \mathbf{x}_B $ 都位于决策面上，因为 $ y(\mathbf{x}_A) = y(\mathbf{x}_B) = 0 $，则有：

$$ \mathbf{w}^T (\mathbf{x}_A - \mathbf{x}_B) = 0 $$

因此，向量 $ \mathbf{w} $ 垂直于位于决策面内的每个向量，$ \mathbf{w} $ 决定了决策面的方向。如果点 $ \mathbf{x} $ 位于决策面上，则 $ y(\mathbf{x}) = 0 $，于是从原点到决策面的法向距离为：

$$ \frac{\mathbf{w}^T \mathbf{x}}{\|\mathbf{w}\|} = -\frac{w_0}{\|\mathbf{w}\|} $$

因此，偏置参数 $ w_0 $ 决定了决策面的位置。对于 $ D = 2 $ 的情况，这些性质在图4.1中有详细说明。

此外，$ y(\mathbf{x}) $ 的值给出了点 $ \mathbf{x} $ 到决策面的垂直距离的符号度量。

#### 扩展到多类分类

对于多类分类问题，可以使用多个判别函数。每个类 $ C_k $ 有自己的判别函数：

$$ y_k(\mathbf{x}) = \mathbf{w}_k^T \mathbf{x} + w_{k0} $$

其中 $ k = 1, \dots, K $。为了方便，可以使用向量表示：

$$ \mathbf{y}(\mathbf{x}) = \mathbf{W}^T \tilde{\mathbf{x}} $$

其中 $ \mathbf{W} $ 是一个矩阵，其第 $ k $ 列包含 $ D+1 $ 维向量 $ \tilde{\mathbf{w}}_k = (w_{k0}, \mathbf{w}_k^T)^T $， $ \tilde{\mathbf{x}} $ 是相应的扩展输入向量 $ (1, \mathbf{x}^T)^T $。

### 结论

在第4.1.1节中，Bishop博士详细阐述了两类分类问题中的线性判别函数。线性判别函数通过将输入向量映射到一个超平面，并根据映射结果进行分类。这种方法简单且具有一定的理论基础，但在实际应用中可能需要结合非线性方法来提高分类效果。
