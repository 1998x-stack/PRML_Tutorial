# 02_4.1.3_Least_squares_for_classification

"""
Lecture: 4_Linear_Models_for_Classification/4.1_Discriminant_Functions
Content: 02_4.1.3_Least_squares_for_classification
"""

### 4.1.3 分类的最小二乘法

在《模式识别与机器学习》（PRML）一书的第4章中，Bishop博士详细介绍了线性分类模型的概念。第4.1节专注于判别函数，并在4.1.3节中讨论了分类问题中的最小二乘法。以下是对4.1.3节内容的详细分析。

#### 最小二乘法应用于分类

在第3章中，我们讨论了参数的线性函数模型，并了解到通过最小化平方误差函数可以得到参数值的简单闭式解。因此，有理由尝试将相同的形式应用于分类问题。

考虑一个具有 $ K $ 类的分类问题，目标向量 $ t $ 采用 1-of-K 二进制编码方案。在这种情况下，使用最小二乘法的一个理由是它可以近似给定输入向量的目标值的条件期望 $ E[t|x] $。对于二进制编码方案，这个条件期望由后验类别概率向量给出。然而，由于线性模型的灵活性有限，这些概率通常近似得很差，甚至可能超出 (0, 1) 的范围。

每个类 $ C_k $ 由其线性模型描述，如下所示：

$$ y_k(x) = \mathbf{w}_k^T \mathbf{x} + w_{k0} $$

其中 $ k = 1, \ldots, K $。我们可以使用向量表示将这些模型方便地组合在一起：

$$ y(x) = \tilde{\mathbf{W}}^T \tilde{\mathbf{x}} $$

其中 $ \tilde{\mathbf{W}} $ 是一个矩阵，其第 $ k $ 列包含 $ D+1 $ 维向量 $ \tilde{\mathbf{w}}_k = (w_{k0}, \mathbf{w}_k^T)^T $， $ \tilde{\mathbf{x}} $ 是相应的扩展输入向量 $ (1, \mathbf{x}^T)^T $。

#### 最小化平方误差函数

我们通过最小化平方误差函数来确定参数矩阵 $ \tilde{\mathbf{W}} $，如同在第3章中对回归所做的那样。考虑一个训练数据集 $ \{ \mathbf{x}_n, \mathbf{t}_n \} $，其中 $ n = 1, \ldots, N $。定义矩阵 $ \mathbf{T} $，其第 $ n $ 行是向量 $ \mathbf{t}_n^T $，以及矩阵 $ \tilde{\mathbf{X}} $，其第 $ n $ 行是 $ \tilde{\mathbf{x}}_n^T $。平方误差函数可以写成：

$$ E_D(\tilde{\mathbf{W}}) = \frac{1}{2} \mathrm{Tr} \left\{ (\tilde{\mathbf{X}} \tilde{\mathbf{W}} - \mathbf{T})^T (\tilde{\mathbf{X}} \tilde{\mathbf{W}} - \mathbf{T}) \right\} $$

对 $ \tilde{\mathbf{W}} $ 求导并重排，可以得到 $ \tilde{\mathbf{W}} $ 的解形式：

$$ \tilde{\mathbf{W}} = (\tilde{\mathbf{X}}^T \tilde{\mathbf{X}})^{-1} \tilde{\mathbf{X}}^T \mathbf{T} = \tilde{\mathbf{X}}^{\dagger} \mathbf{T} $$

其中 $ \tilde{\mathbf{X}}^{\dagger} $ 是矩阵 $ \tilde{\mathbf{X}} $ 的伪逆。由此我们得到判别函数的形式：

$$ y(x) = \tilde{\mathbf{W}}^T \tilde{\mathbf{x}} = \mathbf{T}^T (\tilde{\mathbf{X}}^{\dagger})^T \tilde{\mathbf{x}} $$

#### 最小二乘法的缺陷

尽管最小二乘法给出了判别函数参数的精确闭式解，但它作为判别函数存在一些严重的问题。首先，最小二乘解对异常值缺乏鲁棒性，这在分类应用中也同样适用。此外，平方误差函数惩罚“过于正确”的预测，即那些在决策边界正确一侧距离较远的预测点，这可能导致分类边界的位置显著变化，如图4.4所示。

最小二乘法的问题不仅仅是缺乏鲁棒性，还包括其在处理多类分类问题时表现不佳。图4.5展示了一个合成数据集，其中三类数据在二维输入空间（$ x_1, x_2 $）中分布良好，线性决策边界可以很好地分开这些类。然而，最小二乘解给出了糟糕的结果，只有很小的输入空间区域被分配给绿色类。
