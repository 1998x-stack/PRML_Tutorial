# 04_4.1.5_Relation_to_least_squares

"""
Lecture: 4_Linear_Models_for_Classification/4.1_Discriminant_Functions
Content: 04_4.1.5_Relation_to_least_squares
"""

## 探索PRML.pdf中的知识

PRML (Pattern Recognition and Machine Learning) 是Christopher M. Bishop所著的一本经典教材，涵盖了模式识别和机器学习领域的核心知识。在本书中，第4章介绍了线性分类模型，包括判别函数、最小二乘法分类、Fisher线性判别、感知器算法等。在第4.1.5节中，作者详细讨论了最小二乘法与Fisher线性判别之间的关系。

### 第4.1.5节：最小二乘法与Fisher线性判别的关系

在这一节中，作者探讨了最小二乘法求解线性判别的思路与Fisher线性判别方法的关系，特别是针对两类分类问题。通过对不同目标编码方案的讨论，作者展示了如何通过最小二乘法得到与Fisher线性判别相同的解。

## 深入分析

### 背景介绍

最小二乘法（Least Squares, LS）是一种经典的回归分析方法，通常用于最小化模型预测值与实际观察值之间的平方误差。在分类问题中，最小二乘法可以用来拟合分类模型，尽管这种方法在分类精度和鲁棒性上可能存在一定的缺陷。

Fisher线性判别（Fisher's Linear Discriminant, FLD）是一种用于寻找能够最大化类间方差和最小化类内方差的投影方向的技术。这种方法通过最大化类间散布矩阵与类内散布矩阵之比来找到最佳投影方向。

### 最小二乘法分类与Fisher线性判别

在分类问题中，我们通常使用目标值的1-of-K编码方案，其中K是类别数。然而，作者提出了一种不同的目标编码方案，通过这种方案可以使得最小二乘法的解与Fisher线性判别的解一致。

1. **目标编码方案**：
   - 对于类C1，目标值设为 $ \frac{N}{N1} $，其中N1是类C1中的样本数，N是总样本数。
   - 对于类C2，目标值设为 $ -\frac{N}{N2} $，其中N2是类C2中的样本数。

2. **最小二乘法求解**：
   - 最小二乘法的目标是最小化以下平方误差函数：
     $$
     E = \frac{1}{2} \sum_{n=1}^{N} (w^T x_n + w_0 - t_n)^2
     $$
   - 对误差函数E分别对 $ w_0 $ 和 $ w $ 求导，并令其为零，得到两个方程：
     $$
     \sum_{n=1}^{N} (w^T x_n + w_0 - t_n) = 0
     $$
     $$
     \sum_{n=1}^{N} (w^T x_n + w_0 - t_n) x_n = 0
     $$
   - 通过代入目标编码方案，可以得到偏置 $ w_0 $ 的表达式：
     $$
     w_0 = -w^T m
     $$
     其中，$ m $ 是总数据集的均值：
     $$
     m = \frac{1}{N} \sum_{n=1} x_n = \frac{N1 m1 + N2 m2}{N}
     $$

3. **求解权重向量**：
   - 将上述偏置 $ w_0 $ 代入第二个方程，并通过一些代数运算，最终得到以下形式的方程：
     $$
     (SW + \frac{N1 N2}{N} SB) w = N (m1 - m2)
     $$
     其中，$ SW $ 是类内散布矩阵，$ SB $ 是类间散布矩阵。
   - 通过简化，可以得到权重向量 $ w $ 的解：
     $$
     w \propto S^{-1}_W (m2 - m1)
     $$
     这与Fisher判别的解相同。

### 结论

通过以上分析可以看出，在特定的目标编码方案下，最小二乘法求解的线性判别函数与Fisher线性判别法的解是一致的。这表明，尽管两种方法的出发点和优化目标不同，但在某些情况下，它们可以导出相同的分类决策边界。

这种分析为理解不同机器学习方法之间的关系提供了有价值的视角，有助于我们在实际应用中选择合适的模型和算法。