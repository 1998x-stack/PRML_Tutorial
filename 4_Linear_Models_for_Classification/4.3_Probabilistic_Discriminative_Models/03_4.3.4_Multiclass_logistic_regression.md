# 03_4.3.4_Multiclass_logistic_regression

"""
Lecture: 4_Linear_Models_for_Classification/4.3_Probabilistic_Discriminative_Models
Content: 03_4.3.4_Multiclass_logistic_regression
"""

## 详解PRML中的4.3.4节：多类别逻辑回归

《模式识别与机器学习》（Pattern Recognition and Machine Learning, PRML）是由Christopher M. Bishop所著的一本经典教材，其中第4章涵盖了线性分类模型的内容。在第4.3节，作者介绍了概率判别模型（Probabilistic Discriminative Models）。具体来说，第4.3.4节探讨了多类别逻辑回归（Multiclass Logistic Regression）。以下是对这一节内容的详细分析。

### 多类别逻辑回归的背景

在多类别分类问题中，我们需要将输入向量 $ \phi $ 分配到 $ K $ 个类别中的一个。为了实现这一点，多类别逻辑回归使用Softmax函数将输入变量的线性组合转换为后验概率。Softmax函数是一种归一化指数函数，可以将输入映射到 $[0, 1]$ 区间，并且所有输出的和为1。

### Softmax函数

Softmax函数的形式为：

$$ p(C_k|\phi) = y_k(\phi) = \frac{\exp(a_k)}{\sum_{j} \exp(a_j)} $$

其中，激活函数 $ a_k $ 定义为：

$$ a_k = w_k^T \phi $$

这里 $ w_k $ 是类别 $ C_k $ 的参数向量。通过这种方式，输入向量 $ \phi $ 被映射到一个线性函数，然后通过Softmax函数转换为后验概率。

### 最大似然估计

在多类别逻辑回归中，我们使用最大似然方法来确定模型参数 $ \{w_k\} $。首先，我们定义数据集 $\{(\phi_n, t_n)\}$，其中 $ t_n $ 是一个采用1-of-K编码的目标向量。如果样本 $ n $ 属于类别 $ C_k $，那么 $ t_{nk} = 1 $，否则 $ t_{nk} = 0 $。

#### 似然函数

似然函数可以写成：

$$ p(T|w_1, \ldots, w_K) = \prod_{n=1}^{N} \prod_{k=1}^{K} p(C_k|\phi_n)^{t_{nk}} = \prod_{n=1}^{N} \prod_{k=1}^{K} y_{nk}^{t_{nk}} $$

其中， $ y_{nk} = y_k(\phi_n) $， $ T $ 是一个 $ N \times K $ 的目标变量矩阵，元素为 $ t_{nk} $。

#### 对数似然函数

取似然函数的对数得到对数似然函数：

$$ E(w_1, \ldots, w_K) = - \ln p(T|w_1, \ldots, w_K) = - \sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \ln y_{nk} $$

这就是多类别分类问题的交叉熵误差函数。

### 梯度下降法

为了最小化误差函数，我们使用梯度下降法。误差函数关于参数向量 $ w_j $ 的梯度为：

$$ \nabla_{w_j} E(w_1, \ldots, w_K) = \sum_{n=1}^{N} (y_{nj} - t_{nj}) \phi_n $$

其中， $ y_{nj} $ 是模型对样本 $ n $ 属于类别 $ C_j $ 的预测概率。

### 牛顿-拉弗森法

与二分类逻辑回归类似，我们也可以使用牛顿-拉弗森方法来优化多类别逻辑回归模型。更新公式为：

$$ w_{\text{new}} = w_{\text{old}} - H^{-1} \nabla E(w) $$

其中， $ H $ 是Hessian矩阵，其元素为：

$$ H_{ij} = \frac{\partial^2 E}{\partial w_i \partial w_j} $$

对于多类别逻辑回归，Hessian矩阵的元素为：

$$ \nabla_{w_k} \nabla_{w_j} E(w_1, \ldots, w_K) = - \sum_{n=1}^{N} y_{nk} (I_{kj} - y_{nj}) \phi_n \phi_n^T $$

这里， $ I_{kj} $ 是单位矩阵的元素。

### IRLS算法

迭代重加权最小二乘法（IRLS）是一种基于牛顿-拉弗森方法的优化算法，适用于多类别逻辑回归问题。通过迭代更新参数向量 $ w $，每次使用新的权重向量计算修正后的加权矩阵 $ R $，直到收敛到全局最小值。

### 结论

通过以上分析可以看出，多类别逻辑回归是一种强大的分类模型，通过Softmax函数将输入映射到后验概率，并使用最大似然估计优化参数。它在处理多类别分类问题时表现优异，尤其是在高维数据和类条件密度分布复杂的情况下。掌握多类别逻辑回归的理论和应用，有助于我们在实际问题中选择合适的模型和算法，提高分类和预测的准确性。
