# 00_4.3.1_Fixed_basis_functions

"""
Lecture: 4_Linear_Models_for_Classification/4.3_Probabilistic_Discriminative_Models
Content: 00_4.3.1_Fixed_basis_functions
"""

### 深入解析PRML中的4.3.1节：固定基函数

《模式识别与机器学习》（Pattern Recognition and Machine Learning, PRML）是由Christopher M. Bishop所著的一本经典教材，其中第4章涵盖了线性分类模型的内容。在第4.3节，作者介绍了概率判别模型（Probabilistic Discriminative Models）。具体来说，第4.3.1节探讨了固定基函数的方法。以下是对这一节内容的详细分析。

### 固定基函数的背景

在机器学习中，特别是在线性模型的背景下，基函数（Basis Function）是用于将输入数据转换为特征空间的函数。通过将输入数据映射到一个更高维度或非线性的特征空间，我们可以更容易地处理复杂的模式识别和分类问题。固定基函数意味着这些基函数在训练过程中保持不变，只调整模型参数以最小化误差。

### 固定基函数的形式

固定基函数模型的形式可以写为：

$$ y(x, w) = w_0 + \sum_{j=1}^{M-1} w_j \phi_j(x) $$

其中：
- $ \phi_j(x) $ 是基函数。
- $ w_j $ 是模型参数。
- $ M $ 是基函数的数量。

为了简化表达，我们可以将模型表示为向量的形式：

$$ y(x, w) = w^T \phi(x) $$

其中，$ \phi(x) $ 是基函数向量，包含所有的基函数。

### 常见的基函数类型

1. **多项式基函数**：
   多项式基函数是最简单的一类基函数，通常用于线性回归和多项式回归中。它们的形式为：
   
   $$ \phi_j(x) = x^j $$

2. **高斯基函数**：
   高斯基函数广泛应用于径向基函数网络（Radial Basis Function Networks）和支持向量机（Support Vector Machines）中。它们的形式为：
   
   $$ \phi_j(x) = \exp\left(-\frac{(x - \mu_j)^2}{2s^2}\right) $$
   
   其中，$ \mu_j $ 是基函数的中心，$ s $ 是尺度参数。

3. **Sigmoid基函数**：
   Sigmoid基函数通常用于神经网络中。它们的形式为：
   
   $$ \phi_j(x) = \sigma\left(\frac{x - \mu_j}{s}\right) $$
   
   其中，$ \sigma(a) = \frac{1}{1 + \exp(-a)} $ 是Sigmoid函数。

### 固定基函数的优势

1. **计算简便**：
   固定基函数模型的参数优化通常可以通过线性回归的闭式解来实现，这使得计算非常高效。

2. **理论分析性强**：
   由于模型参数线性化，许多理论结果（如最小二乘解、贝叶斯解）可以直接应用。

3. **非线性映射能力**：
   通过选择合适的基函数，可以将输入数据映射到高维或非线性的特征空间，从而提高模型的表达能力。

### 固定基函数的局限性

1. **基函数选择的依赖性**：
   模型的性能高度依赖于基函数的选择。如果基函数不能有效地表示数据的特征，模型的性能将受到影响。

2. **维数灾难**：
   随着输入维度的增加，所需的基函数数量也会迅速增加，这可能导致计算和存储的挑战。

### 固定基函数在实际中的应用

1. **径向基函数网络（RBFN）**：
   RBFN是一种常见的神经网络模型，广泛应用于模式识别、函数逼近和时间序列预测等领域。其基函数通常为高斯函数。

2. **支持向量机（SVM）**：
   SVM是一种强大的分类和回归工具，通过核技巧将输入数据映射到高维特征空间。常用的核函数包括多项式核、高斯核和Sigmoid核等。

3. **贝叶斯线性回归**：
   在贝叶斯框架下，固定基函数模型可以通过引入先验分布和后验推断来实现，这在处理不确定性和提高模型鲁棒性方面具有优势。
