# 01_4.3.2_Logistic_regression

"""
Lecture: 4_Linear_Models_for_Classification/4.3_Probabilistic_Discriminative_Models
Content: 01_4.3.2_Logistic_regression
"""

## 详解PRML中的4.3.2节：逻辑回归

《模式识别与机器学习》（Pattern Recognition and Machine Learning, PRML）是由Christopher M. Bishop所著的一本经典教材，其中第4章涵盖了线性分类模型的内容。在第4.3节，作者介绍了概率判别模型（Probabilistic Discriminative Models）。具体来说，第4.3.2节探讨了逻辑回归（Logistic Regression）。以下是对这一节内容的详细分析。

### 逻辑回归的背景

逻辑回归是一种广泛应用于二分类问题的判别模型。与生成模型不同，判别模型直接建模条件概率 $p(C_k|x)$，并不关心数据的生成机制。逻辑回归通过将输入变量的线性组合映射到 $[0, 1]$ 区间，用于估计类别的后验概率。

### 逻辑回归模型

逻辑回归模型假设类别 $C_1$ 的后验概率 $p(C_1|\phi)$ 可以表示为：

$$ p(C_1|\phi) = y(\phi) = \sigma(w^T\phi) $$

其中，$\sigma(z)$ 是逻辑Sigmoid函数，定义为：

$$ \sigma(z) = \frac{1}{1 + \exp(-z)} $$

对于类别 $C_2$，我们有：

$$ p(C_2|\phi) = 1 - p(C_1|\phi) $$

#### 线性函数的表示

对于一个M维的特征空间 $\phi$，该模型有M个可调参数。线性函数表示为：

$$ a = w^T\phi $$

其中，$w$ 是权重向量，$\phi$ 是特征向量。

### 最大似然估计

为了确定逻辑回归模型的参数，我们使用最大似然估计。首先，定义数据集 $\{(\phi_n, t_n)\}$，其中 $t_n \in \{0, 1\}$ 表示类别标签，$\phi_n = \phi(x_n)$ 表示特征向量。似然函数可以写为：

$$ p(t|w) = \prod_{n=1}^{N} y_n^{t_n} (1 - y_n)^{1 - t_n} $$

其中，$ y_n = p(C_1|\phi_n) $。取对数后得到对数似然函数：

$$ \ln p(t|w) = \sum_{n=1}^{N} \{t_n \ln y_n + (1 - t_n) \ln (1 - y_n)\} $$

### 交叉熵损失函数

通过取负对数似然函数，我们得到交叉熵损失函数：

$$ E(w) = -\ln p(t|w) = -\sum_{n=1}^{N} \{t_n \ln y_n + (1 - t_n) \ln (1 - y_n)\} $$

### 梯度下降法

为了最小化损失函数，我们可以使用梯度下降法。损失函数关于 $w$ 的梯度为：

$$ \nabla E(w) = \sum_{n=1}^{N} (y_n - t_n) \phi_n $$

其中，$ y_n = \sigma(w^T\phi_n) $。我们可以通过以下迭代更新权重：

$$ w \leftarrow w - \eta \nabla E(w) $$

其中，$\eta$ 是学习率。

### 逻辑回归的优势

1. **计算效率**：逻辑回归的参数优化通常可以通过梯度下降等数值方法高效地实现。
2. **可解释性强**：模型参数具有明确的物理意义，便于解释和分析。
3. **线性模型的推广**：逻辑回归可以看作是线性回归的推广，通过Sigmoid函数将输出限制在 $[0, 1]$ 区间。

### 逻辑回归的局限性

1. **线性可分性假设**：逻辑回归假设数据在特征空间中是线性可分的，对于非线性可分的数据，模型表现较差。
2. **易受异常值影响**：由于损失函数的性质，逻辑回归对异常值较为敏感。

### 扩展到多分类

对于多分类问题，我们可以使用Softmax回归。Softmax回归将输入映射到多个类别，通过Softmax函数将输出归一化，使其表示类别的后验概率。

通过对PRML第4.3.2节的深入解析，我们可以更好地理解逻辑回归在机器学习中的理论基础和实际应用。掌握这些方法有助于我们在实际问题中选择合适的模型和算法，提高分类和预测的准确性。