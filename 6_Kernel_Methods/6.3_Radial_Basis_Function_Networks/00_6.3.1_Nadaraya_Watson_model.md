# 00_6.3.1_Nadaraya-Watson_model

"""
Lecture: 6_Kernel_Methods/6.3_Radial_Basis_Function_Networks
Content: 00_6.3.1_Nadaraya-Watson_model
"""

## 详细分析第6.3.1节：Nadaraya-Watson模型

### 引言
Nadaraya-Watson模型是非参数回归的一种经典方法，通过核密度估计对输入数据进行平滑处理，预测新的输入值。该方法在处理具有高噪声和非线性关系的数据时表现出色。第6.3.1节详细介绍了这一模型的原理和应用。

### 基本概念
Nadaraya-Watson模型基于核密度估计，通过对输入点的加权平均来预测输出值。具体来说，对于一个给定的输入点 $ x $，其预测值 $ \hat{y}(x) $ 可以表示为：
$$ \hat{y}(x) = \frac{\sum_{i=1}^{N} K(x, x_i) y_i}{\sum_{i=1}^{N} K(x, x_i)} $$
其中，$ K(x, x_i) $ 是核函数，表示输入点 $ x $ 和训练数据点 $ x_i $ 之间的相似度，常用的核函数包括高斯核、拉普拉斯核等。

### 核函数的选择
核函数的选择对模型的性能有显著影响。常见的核函数有：
1. **高斯核**：
   $$ K(x, x_i) = \exp \left( -\frac{\|x - x_i\|^2}{2\sigma^2} \right) $$
   高斯核对数据点之间的距离进行了指数衰减，使得距离越近的点对预测的贡献越大。
   
2. **拉普拉斯核**：
   $$ K(x, x_i) = \exp \left( -\frac{\|x - x_i\|}{\sigma} \right) $$
   拉普拉斯核也是一种常用的核函数，与高斯核类似，但其对距离的衰减速度不同。

### 模型训练与预测
Nadaraya-Watson模型的训练过程非常简单，只需保存训练数据即可。在预测时，通过计算核函数来确定输入点与每个训练数据点的相似度，然后进行加权平均得到预测值。

#### 步骤
1. **训练阶段**：保存训练数据 $\{(x_i, y_i)\}_{i=1}^{N}$。
2. **预测阶段**：对于新的输入点 $ x $，计算每个训练点的核函数值 $ K(x, x_i) $，然后通过加权平均计算预测值 $ \hat{y}(x) $。

### 优势与局限
#### 优势
- **非参数性**：Nadaraya-Watson模型不假设数据的分布形式，适用于多种复杂的分布情况。
- **平滑性**：通过核函数的平滑效果，可以有效处理数据中的噪声。

#### 局限
- **计算成本**：对每个预测点，都需要计算与所有训练数据点的核函数值，计算量较大。
- **边界效应**：在输入空间的边界处，由于缺乏足够的邻近点，模型预测可能不准确。

### 实例分析
假设我们有一个一维回归问题，输入变量为 $ x $，目标变量为 $ y $。我们生成一个包含噪声的非线性数据集，并使用Nadaraya-Watson模型进行预测。

#### 数据生成
生成的数据如下：
$$ x_i \sim \mathcal{U}(0, 10) $$
$$ y_i = \sin(x_i) + \epsilon_i $$
其中，$ \epsilon_i \sim \mathcal{N}(0, 0.1) $ 是高斯噪声。

#### 模型训练与预测
1. **训练**：保存生成的数据 $\{(x_i, y_i)\}_{i=1}^{N}$。
2. **预测**：对于新的输入点 $ x $，计算高斯核函数 $ K(x, x_i) $，并进行加权平均得到预测值 $ \hat{y}(x) $。

#### 结果分析
绘制输入 $ x $ 和目标 $ y $ 的散点图，并叠加Nadaraya-Watson模型的预测结果。通过观察，可以发现模型能够很好地捕捉数据的非线性关系，同时对噪声有较好的平滑效果。

### 小结
Nadaraya-Watson模型通过核密度估计，实现了对非线性和高噪声数据的有效建模。其简单直观的计算过程，使得该模型在非参数回归中具有重要应用。然而，计算成本和边界效应仍需在实际应用中加以注意和优化。
