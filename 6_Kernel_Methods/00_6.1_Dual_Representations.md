# 00_6.1_Dual_Representations

"""
Lecture: /6_Kernel_Methods
Content: 00_6.1_Dual_Representations
"""

## 详细分析第6.1节：对偶表示

### 引言
在机器学习中，对偶表示是一种重要的方法，尤其在核方法（Kernel Methods）中，它通过将原始的特征空间映射到一个高维的再生核希尔伯特空间（Reproducing Kernel Hilbert Space, RKHS），使得线性方法能够在高维空间中处理非线性问题。第6.1节详细介绍了对偶表示的基本概念和应用。

### 对偶表示的基本概念
对偶表示的核心思想是利用拉格朗日对偶性，通过对偶变量来表达原始问题的解，从而将问题简化或转化为更容易处理的形式。在核方法中，对偶表示特别有用，因为它允许我们在不显式计算高维映射的情况下使用核函数来计算内积。

#### 线性回归中的对偶表示
考虑一个简单的线性回归问题，目标是最小化以下损失函数：
$$ E(\mathbf{w}) = \frac{1}{2} \|\mathbf{w}\|^2 + \frac{C}{2} \sum_{i=1}^{N} (y_i - \mathbf{w}^T \phi(\mathbf{x}_i))^2 $$
其中，$\mathbf{w}$ 是权重向量，$\phi(\mathbf{x})$ 是输入 $\mathbf{x}$ 的映射，$C$ 是正则化参数。

通过引入拉格朗日乘子 $\alpha_i$，可以将问题转化为对偶形式：
$$ \mathbf{w} = \sum_{i=1}^{N} \alpha_i \phi(\mathbf{x}_i) $$
对偶目标函数为：
$$ \mathcal{L}(\alpha) = \frac{1}{2} \sum_{i,j=1}^{N} \alpha_i \alpha_j \phi(\mathbf{x}_i)^T \phi(\mathbf{x}_j) - \sum_{i=1}^{N} \alpha_i y_i $$

### 核函数
核函数 $ k(\mathbf{x}_i, \mathbf{x}_j) $ 定义为输入向量在高维空间中的内积，即：
$$ k(\mathbf{x}_i, \mathbf{x}_j) = \phi(\mathbf{x}_i)^T \phi(\mathbf{x}_j) $$
常用的核函数包括线性核、多项式核、高斯核（RBF核）等。

#### 高斯核（RBF核）
$$ k(\mathbf{x}_i, \mathbf{x}_j) = \exp\left(-\frac{\|\mathbf{x}_i - \mathbf{x}_j\|^2}{2\sigma^2}\right) $$

### 支持向量机中的对偶表示
支持向量机（SVM）是对偶表示的经典应用之一。SVM的目标是找到一个超平面，将不同类别的数据点最大间隔地分开。其对偶形式为：
$$ \max_\alpha \sum_{i=1}^{N} \alpha_i - \frac{1}{2} \sum_{i,j=1}^{N} \alpha_i \alpha_j y_i y_j k(\mathbf{x}_i, \mathbf{x}_j) $$
其中，$\alpha_i$ 是拉格朗日乘子，$y_i$ 是类别标签。

### 实例分析
假设我们有一个二分类问题，输入变量为 $\mathbf{x} = [x_1, x_2]$，目标变量为 $y $ 。我们使用支持向量机进行分类，并通过核函数进行非线性映射。

#### 数据生成
生成数据如下：
$$ \mathbf{x}_i \sim \mathcal{U}(0, 1) $$
$$ y_i = \begin{cases} 
1, & \text{if } x_1^2 + x_2^2 > 0.5 \\
-1, & \text{otherwise}
\end{cases} $$

#### 模型训练
1. **选择核函数**：选择高斯核函数。
2. **求解对偶问题**：使用优化算法求解对偶问题，得到拉格朗日乘子 $\alpha$。
3. **计算权重向量**：通过 $\mathbf{w} = \sum_{i=1}^{N} \alpha_i y_i \phi(\mathbf{x}_i)$ 计算权重向量。

#### 结果分析
通过比较原始空间和高维空间中的分类效果，可以看到对偶表示和核方法在处理非线性问题时的优势。绘制分类边界和支持向量，展示模型的分类效果。

### 优势与应用
- **处理非线性问题**：通过核函数将非线性问题转化为线性问题。
- **简化计算**：对偶表示使得计算更高效，特别是在高维空间中。
- **广泛应用**：对偶表示在支持向量机、核岭回归、核PCA等多个领域都有广泛应用。

### 小结
对偶表示是机器学习中的重要方法，通过引入核函数和拉格朗日乘子，能够有效处理非线性问题，并简化计算。在核方法和支持向量机等模型中，对偶表示发挥了重要作用。