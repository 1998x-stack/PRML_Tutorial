# 00_6.4.1_Linear_regression_revisited

"""
Lecture: 6_Kernel_Methods/6.4_Gaussian_Processes
Content: 00_6.4.1_Linear_regression_revisited
"""

## 详细分析第6.4.1节：线性回归再探

### 引言
线性回归是统计学和机器学习中的基本方法，用于建模变量之间的线性关系。在第6.4.1节中，线性回归被重新审视，并引入了高斯过程的概念，提供了一种新的视角来理解和扩展线性回归。

### 线性回归的基本公式
在线性回归模型中，我们假设目标变量 $ t $ 和输入变量 $ x $ 之间存在线性关系，模型形式为：
$$ t = w^T x + \epsilon $$
其中，$ w $ 是权重向量，$ \epsilon $ 是高斯噪声，满足 $ \epsilon \sim \mathcal{N}(0, \sigma^2) $。

#### 似然函数
给定训练数据 $\{ (x_n, t_n) \}_{n=1}^{N} $，似然函数表示为：
$$ p(t|X, w, \sigma^2) = \prod_{n=1}^{N} \mathcal{N}(t_n | w^T x_n, \sigma^2) $$
对数似然函数为：
$$ \ln p(t|X, w, \sigma^2) = -\frac{N}{2} \ln(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{n=1}^{N} (t_n - w^T x_n)^2 $$

#### 最大似然估计
通过最大化对数似然函数，可以得到参数 $ w $ 和 $ \sigma^2 $ 的最大似然估计：
$$ w_{ML} = (X^T X)^{-1} X^T t $$
$$ \sigma^2_{ML} = \frac{1}{N} \sum_{n=1}^{N} (t_n - w_{ML}^T x_n)^2 $$

### 高斯过程视角
高斯过程（Gaussian Processes, GPs）提供了一种灵活的方式来建模函数分布，是对线性回归的扩展。高斯过程假设任何有限维输入集合对应的输出服从多元高斯分布。

#### 核函数
高斯过程使用核函数 $ k(x, x') $ 来定义输入点之间的相似性。常见的核函数包括：
1. **线性核**：
   $$ k(x, x') = x^T x' $$
2. **高斯核**（RBF核）：
   $$ k(x, x') = \exp \left( -\frac{\|x - x'\|^2}{2l^2} \right) $$
3. **多项式核**：
   $$ k(x, x') = (x^T x' + c)^d $$

### 高斯过程回归
在高斯过程回归中，假设目标函数 $ f(x) $ 服从一个高斯过程：
$$ f(x) \sim \mathcal{GP}(0, k(x, x')) $$
给定训练数据集 $ \{(x_n, t_n)\}_{n=1}^{N} $，目标变量的联合分布为：
$$ p(t|X) = \mathcal{N}(0, K + \sigma^2 I) $$
其中， $ K $ 是核矩阵， $ K_{ij} = k(x_i, x_j) $。

#### 预测分布
对于新的输入 $ x_* $，预测分布也是高斯分布，均值和协方差分别为：
$$ \mu_* = k_*^T (K + \sigma^2 I)^{-1} t $$
$$ \sigma^2_* = k(x_*, x_*) - k_*^T (K + \sigma^2 I)^{-1} k_* $$
其中， $ k_* $ 是新的输入与训练数据之间的核向量。

### 实例分析
假设我们有一个一维回归问题，输入变量为 $ x $，目标变量为 $ t $。生成数据如下：
$$ x_i \sim \mathcal{U}(0, 10) $$
$$ t_i = \sin(x_i) + \epsilon_i $$
其中， $ \epsilon_i \sim \mathcal{N}(0, 0.1) $。

#### 数据可视化
绘制输入 $ x $ 和目标 $ t $ 的散点图。

#### 训练高斯过程模型
1. **构建核矩阵** $ K $。
2. **计算预测均值和方差**。

#### 结果分析
通过可视化预测均值和方差，可以看到高斯过程模型不仅捕捉了数据的非线性关系，还量化了预测的不确定性。

### 小结
线性回归的重新审视通过引入高斯过程，扩展了模型的应用范围。高斯过程回归提供了一种灵活且强大的方法来建模复杂的函数关系，并有效量化预测的不确定性。
