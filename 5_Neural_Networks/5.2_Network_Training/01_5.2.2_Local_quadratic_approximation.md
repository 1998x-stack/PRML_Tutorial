# 01_5.2.2_Local_quadratic_approximation

"""
Lecture: 5_Neural_Networks/5.2_Network_Training
Content: 01_5.2.2_Local_quadratic_approximation
"""

### 5.2.2 局部二次近似——极其详细和深入分析

#### 前言

在神经网络训练过程中，为了更好地理解优化问题及其解决方法，可以考虑误差函数的局部二次近似。这一方法通过泰勒展开，将误差函数在权重空间的某一点附近进行近似，从而简化优化过程。接下来，我们将对局部二次近似进行极其详细和深入的分析。

#### 局部二次近似的理论基础

为了简化误差函数 $ E(\mathbf{w}) $ 的优化过程，我们可以在权重空间的某个点 $ \hat{\mathbf{w}} $ 处对其进行泰勒展开。假设 $ E(\mathbf{w}) $ 是在 $ \hat{\mathbf{w}} $ 处的泰勒展开形式如下：
$$ E(\mathbf{w}) \approx E(\hat{\mathbf{w}}) + (\mathbf{w} - \hat{\mathbf{w}})^T \mathbf{b} + \frac{1}{2} (\mathbf{w} - \hat{\mathbf{w}})^T \mathbf{H} (\mathbf{w} - \hat{\mathbf{w}}) $$
其中，梯度 $ \mathbf{b} $ 和 Hessian 矩阵 $ \mathbf{H} $ 分别表示为：
$$ \mathbf{b} = \nabla E \bigg|_{\mathbf{w}=\hat{\mathbf{w}}} $$
$$ \mathbf{H} = \nabla \nabla E \bigg|_{\mathbf{w}=\hat{\mathbf{w}}} $$

在这个展开式中，我们忽略了三次及更高次的项，从而简化计算。对于 $ \mathbf{w} $ 足够接近 $ \hat{\mathbf{w}} $ 的情况，这个近似可以较好地描述误差函数及其梯度。

#### 最小点的二次近似

考虑在误差函数的某个最小点 $ \mathbf{w}^* $ 附近的局部二次近似。此时，由于在最小点处梯度为零，即 $ \nabla E(\mathbf{w}^*) = 0 $，因此展开式中的线性项消失，误差函数可以简化为：
$$ E(\mathbf{w}) \approx E(\mathbf{w}^*) + \frac{1}{2} (\mathbf{w} - \mathbf{w}^*)^T \mathbf{H} (\mathbf{w} - \mathbf{w}^*) $$

为了更好地理解这种近似，可以考虑 Hessian 矩阵的特征值分解。设 Hessian 矩阵 $ \mathbf{H} $ 的特征值分解为：
$$ \mathbf{H} \mathbf{u}_i = \lambda_i \mathbf{u}_i $$
其中， $ \mathbf{u}_i $ 是特征向量， $ \lambda_i $ 是对应的特征值。特征向量 $ \mathbf{u}_i $ 构成了一个完备的正交基（参见附录C），即：
$$ \mathbf{u}_i^T \mathbf{u}_j = \delta_{ij} $$

我们可以将 $ \mathbf{w} - \mathbf{w}^* $ 展开为这些特征向量的线性组合：
$$ \mathbf{w} - \mathbf{w}^* = \sum_i \alpha_i \mathbf{u}_i $$

将其代入误差函数的近似式，并利用特征值分解，可以得到：
$$ E(\mathbf{w}) \approx E(\mathbf{w}^*) + \frac{1}{2} \sum_i \lambda_i \alpha_i^2 $$

#### Hessian 矩阵的几何解释

在新的坐标系中，误差函数的等高线将是以最小点为中心的椭圆，其轴与 Hessian 矩阵的特征向量对齐，长度与特征值的平方根成反比。这意味着在特征值较小的方向上，误差函数变化较缓，而在特征值较大的方向上，误差函数变化较快。

#### Hessian 矩阵的正定性

一个矩阵 $ \mathbf{H} $ 如果对任意非零向量 $ \mathbf{v} $ 满足 $ \mathbf{v}^T \mathbf{H} \mathbf{v} > 0 $，则称其为正定矩阵。在局部二次近似中，如果 Hessian 矩阵是正定的，则误差函数在该点附近有一个局部最小值。

#### 实践中的应用

局部二次近似在实际应用中的一个重要用途是优化算法的设计。例如，在 Newton-Raphson 方法中，通过使用 Hessian 矩阵的逆，可以显著加快收敛速度。对于复杂的误差函数，通过局部二次近似，可以在每次迭代中有效地找到前进方向，从而提高优化效率。

此外，局部二次近似还可以用于分析优化问题的困难程度。通过特征值分解，可以判断误差函数在不同方向上的曲率，从而选择合适的优化策略。例如，对于高维度的神经网络，可以采用共轭梯度法等优化算法，以避免在特征值较小的方向上收敛缓慢的问题。

#### 总结

局部二次近似是一种强大的工具，可以帮助我们更好地理解和解决神经网络中的优化问题。通过对误差函数进行泰勒展开，我们可以简化计算，并利用 Hessian 矩阵的特征值分解，深入分析误差函数的几何性质。理解和应用局部二次近似，可以显著提高优化算法的效率和稳定性，从而提升神经网络的训练效果。