# 02_5.7.3_Bayesian_neural_networks_for_classification

"""
Lecture: 5_Neural_Networks/5.7_Bayesian_Neural_Networks
Content: 02_5.7.3_Bayesian_neural_networks_for_classification
"""

## 详细分析第5.7.3节：用于分类的贝叶斯神经网络

### 引言
贝叶斯神经网络（BNN）在分类任务中表现出色，主要因为其能够量化预测中的不确定性，并通过对模型参数的后验分布进行推断，提供更加稳健的预测。第5.7.3节讨论了如何将贝叶斯方法应用于神经网络分类任务中。

### 贝叶斯分类模型
贝叶斯分类器的核心思想是利用贝叶斯公式来计算类别后验概率。对于一个输入向量$x$，模型输出属于类别$C_k$的后验概率为：
$$ p(C_k|x, D) = \frac{p(x|C_k)p(C_k)}{p(x)} $$
其中，$ p(x|C_k) $是类别$C_k$的似然，$ p(C_k) $是类别先验，$ p(x) $是证据。

#### 神经网络输出
在神经网络分类器中，输出层通常采用Softmax函数，将网络的输出$a_k$转化为类别的概率分布：
$$ y_k = \frac{\exp(a_k)}{\sum_{j}\exp(a_j)} $$
这样，$ y_k $ 表示输入 $ x $ 属于类别 $ k $ 的概率。

### 贝叶斯神经网络的训练
训练贝叶斯神经网络涉及到对参数的后验分布进行估计。由于精确的后验分布难以计算，通常使用近似方法，如拉普拉斯近似或变分推断。下面介绍两种常见的方法：

#### 拉普拉斯近似
拉普拉斯近似通过将后验分布近似为高斯分布，简化了计算过程。具体步骤如下：
1. **找到后验分布的最大值** $ w_{MAP} $，通过最大化后验对数来实现：
   $$ \ln p(w|D) = -E(w) + const $$
   其中 $ E(w) $ 是负对数似然。
2. **计算Hessian矩阵** $ H $，这是误差函数对参数 $ w $ 的二阶导数矩阵：
   $$ H = \nabla\nabla E(w) $$
3. **高斯近似** $ q(w|D) = N(w|w_{MAP}, H^{-1}) $

#### 变分推断
变分推断通过优化一个可变分布 $ q(w) $ 来近似后验分布 $ p(w|D) $，使得 $ q(w) $ 和 $ p(w|D) $ 之间的KL散度最小。变分推断的步骤如下：
1. **选择变分分布** $ q(w) $，通常选择一个简单的分布如高斯分布。
2. **优化变分参数**，通过最大化变分下界（ELBO）来进行：
   $$ \text{ELBO} = \mathbb{E}_{q(w)}[\ln p(D|w)] - \text{KL}(q(w) || p(w)) $$

### 预测分布
对于新的输入 $ x^* $，贝叶斯神经网络的预测分布通过对参数进行边缘化处理得到：
$$ p(C_k|x^*, D) = \int p(C_k|x^*, w) p(w|D) \, dw $$
在拉普拉斯近似下，这个积分可以通过采样或者变分方法来近似计算。

### 不确定性量化
贝叶斯神经网络的一个显著优势是其能够量化预测的不确定性，这在很多实际应用中非常重要。模型输出的概率分布不仅提供了最可能的预测，还反映了模型对预测的置信程度。

### 小结
贝叶斯神经网络通过对参数的后验分布进行建模和推断，能够提供更加稳健的预测，并有效量化预测中的不确定性。拉普拉斯近似和变分推断是两种常用的近似方法，它们使得贝叶斯方法在复杂模型中的应用成为可能。
