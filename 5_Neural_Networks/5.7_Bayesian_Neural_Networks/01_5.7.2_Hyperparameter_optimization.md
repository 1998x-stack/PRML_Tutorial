# 01_5.7.2_Hyperparameter_optimization

"""
Lecture: 5_Neural_Networks/5.7_Bayesian_Neural_Networks
Content: 01_5.7.2_Hyperparameter_optimization
"""

## 详细分析第5.7.2节：超参数优化

### 引言
在贝叶斯神经网络中，超参数（如正则化参数α和噪声精度β）对模型的性能有重要影响。超参数优化旨在通过最大化边际似然（evidence）来选择最佳的超参数值。第5.7.2节介绍了这种优化过程，特别是使用Laplace近似来简化计算的具体方法。

### 边际似然和证据框架
边际似然（或称evidence）用于评估模型的整体适应性，通过对模型参数进行边缘化来计算。边际似然定义为：
$$ p(D|α, β) = \int p(D|w, β)p(w|α) \, dw $$
其中，$ p(D|w, β) $ 是似然函数，$ p(w|α) $ 是参数的先验分布。

#### Laplace近似
由于直接计算边际似然的积分通常是不可行的，Laplace近似是一种常用的方法，它通过将后验分布近似为高斯分布来简化计算。具体步骤如下：

1. **找到后验分布的最大值** $ w_{MAP} $：
   最大化后验分布的对数等价于最小化正则化的平方和误差函数：
   $$ \ln p(w|D) = -\frac{\alpha}{2} w^T w - \frac{\beta}{2} \sum_{n=1}^{N} \{y(x_n, w) - t_n\}^2 + const $$
   使用标准的非线性优化算法如共轭梯度法可以找到$ w_{MAP} $。

2. **计算Hessian矩阵** $ A $：
   Hessian矩阵是负对数后验分布的二阶导数矩阵：
   $$ A = -\nabla\nabla \ln p(w|D, α, β) = \alpha I + β H $$
   其中，$ H $ 是平方和误差函数对w的二阶导数矩阵。

3. **高斯近似**：
   $$ q(w|D) = N(w|w_{MAP}, A^{-1}) $$

#### 证据的计算
使用Laplace近似后，可以通过以下公式计算对数边际似然：
$$ \ln p(D|α, β) \approx -E(w_{MAP}) - \frac{1}{2} \ln |A| + \frac{W}{2} \ln α + \frac{N}{2} \ln β - \frac{N}{2} \ln (2π) $$
其中，$ E(w_{MAP}) $ 是正则化的误差函数在$ w_{MAP} $处的值，W是参数w的总数。

### 超参数优化过程
为了选择最佳的超参数α和β，我们需要最大化对数边际似然。具体步骤如下：

1. **最大化对数边际似然**：
   - 对α的优化：
     $$ \alpha = \frac{\gamma}{w_{MAP}^T w_{MAP}} $$
     其中，γ表示有效参数数量，定义为：
     $$ \gamma = \sum_{i=1}^{W} \frac{λ_i}{α + λ_i} $$
     其中，$ λ_i $ 是Hessian矩阵的特征值。

   - 对β的优化：
     $$ \frac{1}{β} = \frac{1}{N - \gamma} \sum_{n=1}^{N} \{y(x_n, w_{MAP}) - t_n\}^2 $$

2. **迭代优化**：
   需要在重新估计超参数α和β与更新后验分布之间交替进行迭代，以找到最优解。由于后验分布的多模态性，不同的初始条件可能会导致不同的局部最优解。

### 模型比较
为了比较不同的模型（例如具有不同隐藏单元数量的神经网络），需要计算模型证据$ p(D) $。可以通过将迭代优化得到的α和β值代入上述对数边际似然公式来近似计算证据。此外，可以通过对α和β进行边缘化处理，进一步提高证据的计算精度。

### 结论
超参数优化在贝叶斯神经网络中具有重要作用，通过最大化边际似然来选择最优的超参数值，可以显著提高模型的性能。Laplace近似提供了一种有效的途径，使得复杂的积分计算变得可行，并使得贝叶斯神经网络在处理大规模数据和复杂模型时表现出色。