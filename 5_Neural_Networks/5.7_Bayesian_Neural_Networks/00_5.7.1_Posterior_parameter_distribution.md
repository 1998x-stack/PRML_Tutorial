# 00_5.7.1_Posterior_parameter_distribution

"""
Lecture: 5_Neural_Networks/5.7_Bayesian_Neural_Networks
Content: 00_5.7.1_Posterior_parameter_distribution
"""

## 详细分析第5.7.1节：后验参数分布

### 引言
在贝叶斯神经网络中，我们关注的重点是如何通过后验分布来决定网络参数（权重和偏置）。传统的最大似然估计方法只找到一个最佳参数组合，而贝叶斯方法则考虑参数的不确定性，通过对参数分布进行边缘化处理来进行预测。第5.7.1节详细讨论了这一过程，尤其是后验参数分布的计算方法。

### 条件分布和先验分布
首先，假设我们要预测一个单一的连续目标变量t，它依赖于输入向量x。我们假设条件分布p(t|x)是高斯分布，其均值由神经网络模型y(x, w)给出，精度（方差的倒数）为β：
$$ p(t|x, w, β) = N(t|y(x, w), β^{-1}) $$
此外，我们选择一个对权重w的先验分布，同样假设为高斯分布，其形式为：
$$ p(w|α) = N(w|0, α^{-1}I) $$

### 似然函数和后验分布
对于一个包含N个观测值的数据集D = \{(x_1, t_1), (x_2, t_2), ..., (x_N, t_N)\}，似然函数表示为：
$$ p(D|w, β) = \prod_{n=1}^{N} N(t_n|y(x_n, w), β^{-1}) $$
于是，后验分布可以表示为：
$$ p(w|D, α, β) \propto p(w|α) p(D|w, β) $$

### 拉普拉斯近似
由于网络函数y(x, w)对w的非线性依赖性，后验分布通常是非高斯分布。为了简化计算，我们可以使用拉普拉斯近似，将后验分布近似为一个以后验分布模式为中心的高斯分布。具体步骤如下：

1. **找到后验分布的局部最大值** $w_{MAP}$：
   这一步通常通过标准的非线性优化算法如共轭梯度法实现，使用误差反向传播算法计算所需的导数。最大化后验对数可以表示为：
   $$ \ln p(w|D) = -\frac{\alpha}{2} w^T w - \frac{\beta}{2} \sum_{n=1}^{N} \{y(x_n, w) - t_n\}^2 + const $$

2. **计算负对数后验分布的二阶导数矩阵（Hessian矩阵）** $A$：
   $$ A = -\nabla\nabla \ln p(w|D, α, β) = \alpha I + \beta H $$
   其中，$H$是平方和误差函数对w的二阶导数矩阵。

3. **建立高斯近似**：
   $$ q(w|D) = N(w|w_{MAP}, A^{-1}) $$

### 预测分布
为了得到预测分布，需要对后验分布进行边缘化处理。即使采用高斯近似，积分仍然是不可解析的。我们假设后验分布的方差较小，从而可以对网络函数进行泰勒展开，只保留线性项：
$$ y(x, w) \approx y(x, w_{MAP}) + g^T (w - w_{MAP}) $$
其中，
$$ g = \nabla_w y(x, w) |_{w=w_{MAP}} $$
最终，预测分布可以表示为：
$$ p(t|x, D) \approx \int p(t|x, w) q(w|D) dw $$

### 小结
贝叶斯神经网络通过对参数的后验分布进行建模，能够更全面地处理参数不确定性，提高预测的鲁棒性和可靠性。拉普拉斯近似方法提供了一种有效的途径，将复杂的后验分布近似为高斯分布，简化了计算过程。