# 04_5.5.5_Training_with_transformed_data

"""
Lecture: 5_Neural_Networks/5.5_Regularization_in_Neural_Networks
Content: 04_5.5.5_Training_with_transformed_data
"""

## 详细分析第5.5.5节：使用转换后的数据进行训练

### 引言
在深度学习中，正则化是一种防止模型过拟合的重要技术。第5.5.5节讨论了一种特定的正则化方法，即通过使用转换后的数据进行训练，以增强模型对各种变换的不变性。

### 转换数据的训练
我们知道，通过扩展训练集并使用原始输入模式的转换版本，可以鼓励模型对一组变换的不变性。此方法与切向传播技术密切相关。

#### 理论基础
假设变换由单个参数ξ控制，并由函数s(x, ξ)描述，其中s(x, 0) = x。考虑一个平方和误差函数，对于未转换的输入，误差函数可以表示为：
$$ E = \frac{1}{2} \iint \{y(x) - t\}^2 p(t|x)p(x) \,dx\,dt $$
如果考虑一个无限数量的每个数据点的副本，并且每个副本都通过参数ξ的分布p(ξ)扰动，则扩展数据集上的误差函数可以表示为：
$$ \tilde{E} = \frac{1}{2} \iiint \{y(s(x, ξ)) - t\}^2 p(t|x)p(x)p(ξ) \,dx\,dt\,dξ $$

#### 泰勒展开
假设分布p(ξ)的均值为零且方差较小，因此我们只考虑原始输入向量的小变换。可以将变换函数展开为ξ的泰勒级数：
$$ s(x, ξ) = s(x, 0) + ξ \frac{\partial s(x, ξ)}{\partial ξ} \Big|_{\xi=0} + \frac{ξ^2}{2} \frac{\partial^2 s(x, ξ)}{\partial ξ^2} \Big|_{\xi=0} + O(ξ^3) $$
其中τ表示一阶导数，τ′表示二阶导数。因此，模型函数可以展开为：
$$ y(s(x, ξ)) = y(x) + ξτ^T∇y(x) + \frac{ξ^2}{2} \left[(τ′)^T ∇y(x) + τ^T ∇∇y(x)τ \right] + O(ξ^3) $$

#### 平均误差函数的展开
将泰勒展开代入平均误差函数并展开，可以得到：
$$ \tilde{E} = E + λΩ $$
其中λ表示E[ξ²]，正则化项Ω形式为：
$$ Ω = \int \left\{ \frac{1}{2} \left[(τ′)^T ∇y(x) + τ^T ∇∇y(x)τ\right] + (τ^T ∇y(x))^2 \right\} p(x) \,dx $$

### 数据扩展和计算成本
使用扩展数据可以显著改善模型的泛化能力，但也会带来计算成本的增加。对于顺序训练算法，可以在每次将输入模式呈现给模型之前进行变换；对于批处理方法，可以通过独立变换每个数据点的多个副本来实现类似效果。

### 小结
通过使用转换后的数据进行训练，可以有效地增强模型对各种变换的不变性，从而提高模型的泛化能力。尽管这种方法计算成本较高，但其在提高模型性能方面的效果是显著的。