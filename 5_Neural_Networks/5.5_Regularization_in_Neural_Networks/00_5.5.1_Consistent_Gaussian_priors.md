# 00_5.5.1_Consistent_Gaussian_priors

"""
Lecture: 5_Neural_Networks/5.5_Regularization_in_Neural_Networks
Content: 00_5.5.1_Consistent_Gaussian_priors
"""

### 5.5.1 一致高斯先验——极其详细和深入分析

#### 前言

在神经网络的正则化过程中，高斯先验的使用是一种常见的方法。为了确保网络映射的一致性，我们需要选择适当的先验分布。本节将详细讨论一致高斯先验的理论基础、数学形式及其在神经网络正则化中的应用和优势。

#### 理论基础

一致高斯先验的基本思想是确保先验分布与网络映射的某些比例变换性质一致。为了说明这一点，我们考虑一个具有两层权重和线性输出单元的多层感知机网络，该网络执行从输入变量集 $\{x_i\}$ 到输出变量集 $\{y_k\}$ 的映射。隐藏单元的激活函数形式为：
$$ z_j = h\left(\sum_i w_{ji} x_i + w_{j0}\right) $$
输出单元的激活函数形式为：
$$ y_k = \sum_j w_{kj} z_j + w_{k0} $$

#### 线性变换的一致性

假设我们对输入数据进行线性变换：
$$ x_i \rightarrow \tilde{x}_i = ax_i + b $$
为了使网络的映射保持不变，我们需要对输入到隐藏层单元的权重和偏置进行相应的线性变换：
$$ w_{ji} \rightarrow \tilde{w}_{ji} = \frac{w_{ji}}{a} $$
$$ w_{j0} \rightarrow \tilde{w}_{j0} = w_{j0} - \frac{b}{a} \sum_i w_{ji} $$

类似地，对输出变量进行线性变换：
$$ y_k \rightarrow \tilde{y}_k = c y_k + d $$
可以通过对第二层的权重和偏置进行变换来实现：
$$ w_{kj} \rightarrow \tilde{w}_{kj} = c w_{kj} $$
$$ w_{k0} \rightarrow \tilde{w}_{k0} = c w_{k0} + d $$

为了确保正则化项在这些变换下保持不变，我们需要选择一种在权重重新缩放和偏置平移下保持不变的正则化项。这种正则化项形式为：
$$ \frac{\lambda_1}{2} \sum_{w \in W1} w^2 + \frac{\lambda_2}{2} \sum_{w \in W2} w^2 $$
其中，$ W1 $ 表示第一层的权重集合，$ W2 $ 表示第二层的权重集合，且偏置不包含在求和中。

#### 高斯先验分布

上述正则化项对应的先验分布形式为：
$$ p(w|\alpha_1, \alpha_2) \propto \exp\left(-\frac{\alpha_1}{2} \sum_{w \in W1} w^2 - \frac{\alpha_2}{2} \sum_{w \in W2} w^2 \right) $$
需要注意的是，这种形式的先验是不适定的（improper），因为偏置参数是无限制的。在贝叶斯框架中，不适定先验会导致证据为零，从而在选择正则化系数和模型比较时带来困难。因此，通常会为偏置引入单独的先验，并赋予其自己的超参数。

#### 实际应用中的一致高斯先验

在实际应用中，一致高斯先验通过对权重施加独立的高斯分布来控制网络的复杂度，从而防止过拟合。通过绘制从先验分布中抽样得到的网络函数，可以直观地观察到四个超参数对网络函数的影响。

为了确保正则化参数的一致性，我们需要在进行权重变换时重新调整正则化参数：
$$ \lambda_1 \rightarrow a^{1/2} \lambda_1 $$
$$ \lambda_2 \rightarrow c^{-1/2} \lambda_2 $$

#### 总结

一致高斯先验是一种有效的正则化方法，通过确保先验分布与网络映射的一致性来控制网络的复杂度。理解和应用一致高斯先验，对于设计和实现高效的神经网络正则化方法至关重要。通过选择适当的先验分布和正则化项，可以提高模型的泛化能力，防止过拟合，从而在实际应用中获得更好的性能。