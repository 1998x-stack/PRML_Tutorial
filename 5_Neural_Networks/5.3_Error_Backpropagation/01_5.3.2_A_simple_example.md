# 01_5.3.2_A_simple_example

"""
Lecture: 5_Neural_Networks/5.3_Error_Backpropagation
Content: 01_5.3.2_A_simple_example
"""

### 5.3.2 一个简单示例——极其详细和深入分析

#### 前言

在上一节中，我们讨论了反向传播算法的理论基础和数学推导。本节将通过一个具体的示例，来详细说明反向传播算法的应用过程。这个示例被选中不仅因为其简单性，还因为其在实际应用中的重要性。许多文献中报道的神经网络应用都使用了这种类型的网络。

#### 网络结构

我们考虑一个两层的前馈神经网络，如图5.1所示。该网络的输出单元使用线性激活函数，而隐藏单元使用逻辑sigmoid激活函数。具体来说，输出单元的激活函数为：
$$ y_k = a_k $$
而隐藏单元的激活函数为：
$$ h(a) \equiv \tanh(a) $$
其中：
$$ \tanh(a) = \frac{e^a - e^{-a}}{e^a + e^{-a}} $$
这种激活函数的一个有用特性是其导数形式非常简单：
$$ h'(a) = 1 - h(a)^2 $$

我们考虑一个标准的平方和误差函数，对于模式 $ n $ ，其误差定义为：
$$ E_n = \frac{1}{2} \sum_{k=1}^{K} (y_{k} - t_{k})^2 $$
其中， $ y_k $ 是输出单元 $ k $ 的激活， $ t_k $ 是对应的目标值，针对特定的输入模式 $ x_n $。

#### 前向传播

对于训练集中的每个模式，我们首先进行前向传播计算输出：
$$ a_j = \sum_{i=0}^{D} w_{ji}^{(1)} x_i $$
$$ z_j = \tanh(a_j) $$
$$ y_k = \sum_{j=0}^{M} w_{kj}^{(2)} z_j $$

#### 反向传播

接下来，我们计算每个输出单元的 $\delta$ 值：
$$ \delta_k = y_k - t_k $$
然后，我们将这些 $\delta$ 值反向传播到隐藏单元：
$$ \delta_j = (1 - z_j^2) \sum_{k=1}^{K} w_{kj} \delta_k $$

最后，计算关于第一层和第二层权重的导数：
$$ \frac{\partial E_n}{\partial w_{ji}^{(1)}} = \delta_j x_i $$
$$ \frac{\partial E_n}{\partial w_{kj}^{(2)}} = \delta_k z_j $$

#### 实际应用

在实际应用中，反向传播算法被用于计算所有样本的梯度，并通过梯度下降法更新权重。对于批处理方法，通过对训练集中的所有模式重复上述步骤，然后对所有模式的结果进行求和，可以得到总误差 $ E $ 的导数：
$$ \frac{\partial E}{\partial w_{ji}} = \sum_{n} \frac{\partial E_n}{\partial w_{ji}} $$

#### 数值验证

为了确保反向传播实现的正确性，通常会将反向传播计算的导数与数值微分方法计算的导数进行比较。数值微分方法通过扰动每个权重并计算误差变化，来近似导数：
$$ \frac{\partial E_n}{\partial w_{ji}} \approx \frac{E_n(w_{ji} + \epsilon) - E_n(w_{ji})}{\epsilon} $$
这种方法尽管计算量大，但对于验证导数计算的正确性非常有用。

#### 总结

通过一个简单的两层网络示例，我们详细说明了反向传播算法的前向传播和反向传播过程。反向传播算法因其高效性和准确性，在神经网络训练中得到了广泛应用。理解反向传播算法的实际应用，对于设计和实现高效的神经网络训练方法至关重要。通过结合数值验证方法，可以确保反向传播算法的实现是正确的，从而提高模型的性能和稳定性。