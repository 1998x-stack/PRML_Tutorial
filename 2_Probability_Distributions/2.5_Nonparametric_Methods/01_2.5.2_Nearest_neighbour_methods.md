# 01_2.5.2_Nearest-neighbour_methods

"""
Lecture: 2_Probability_Distributions/2.5_Nonparametric_Methods
Content: 01_2.5.2_Nearest-neighbour_methods
"""

### PDF 探索和详细分析

#### 最近邻方法（Nearest-Neighbour Methods）

在2.5.2节中，讨论了最近邻方法的概念及其在概率密度估计和分类中的应用。最近邻方法是一种非参数方法，用于在数据空间中找到给定点的局部密度或进行分类。

#### 定义与背景

1. **最近邻方法的概念**：
   最近邻方法（Nearest-Neighbour Methods）通过计算数据空间中离目标点最近的K个点来估计局部密度或进行分类。它的基本思想是，通过这些邻近点的分布来推测目标点的特性。

2. **密度估计**：
   最近邻密度估计的基本公式为：
   $$
   p(x) = \frac{K}{NV}
   $$
   其中，$ K $ 是包含在体积 $ V $ 内的数据点数，$ N $ 是总样本数，$ V $ 是包含 $ K $ 个点的球体积。

3. **带宽选择**：
   与核密度估计中的固定带宽不同，最近邻方法根据数据点的分布动态调整带宽（体积）。在数据密集区域，较小的带宽可以避免过度平滑；在数据稀疏区域，较大的带宽可以减少噪声影响。

4. **K值选择**：
   - 较小的 $ K $ 值会导致估计结果对噪声敏感，密度函数呈现“尖锐”特性。
   - 较大的 $ K $ 值则会导致过度平滑，可能丢失数据的局部结构。
   - 最优的 $ K $ 值通常介于两者之间，需要通过交叉验证等方法进行选择。

5. **分类应用**：
   在分类任务中，K最近邻分类器（K-Nearest Neighbour, KNN）通过计算目标点的K个最近邻点，并根据这些邻点的类别来预测目标点的类别。具体步骤如下：
   - 对于每个测试点，找到训练数据集中离它最近的K个点。
   - 根据这K个点的类别，使用投票法决定测试点的类别。
   - 当K=1时，该方法称为最近邻规则。

#### 应用示例

1. **密度估计**：
   假设我们有一组数据，利用最近邻方法估计其概率密度。首先选择一个K值，然后计算每个点的局部密度，最后求和得到总体的概率密度估计。

2. **分类任务**：
   对于分类任务，利用KNN算法预测测试点的类别。通过选择合适的K值，可以在不做任何分布假设的情况下，实现对未知样本的分类。

#### 优缺点

- **优点**：
  - 简单直观，容易实现。
  - 无需对数据进行参数化建模，可以处理各种形状的分布。
- **缺点**：
  - 计算复杂度较高，特别是在高维数据情况下。
  - 需要存储所有的训练数据，对内存要求较大。

### 结论

最近邻方法是一种强大的非参数方法，可以在不做任何分布假设的情况下，对数据的概率密度进行估计或进行分类。它通过选择合适的K值，可以灵活地适应各种数据结构，为概率密度估计和分类提供了有效的工具。