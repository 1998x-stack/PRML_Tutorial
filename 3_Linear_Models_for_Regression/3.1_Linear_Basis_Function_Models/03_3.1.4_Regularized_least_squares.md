# 03_3.1.4_Regularized_least_squares

"""
Lecture: 3_Linear_Models_for_Regression/3.1_Linear_Basis_Function_Models
Content: 03_3.1.4_Regularized_least_squares
"""

### Task 1: 探索PDF

已经完成对PDF的探索。

### Task 2: 深入详细分析

**3.1.4 Regularized Least Squares**

#### 概述
正则化最小二乘法是通过在误差函数中添加正则化项来控制过拟合的方法。这样，整个误差函数变为需要最小化的总误差函数。正则化项的引入不仅能减少过拟合，还能在一定程度上提高模型的泛化能力。

#### 详细分析

**正则化项**

正则化项通常形式为权重向量元素的平方和：

$$ E_W(w) = \frac{1}{2} w^T w $$

整个误差函数变为：

$$ E(w) = \frac{1}{2} \sum_{n=1}^N \{ t_n - w^T \phi(x_n) \}^2 + \frac{\lambda}{2} w^T w $$

其中，$\lambda$ 是正则化系数，控制数据依赖误差 $E_D(w)$ 和正则化项 $E_W(w)$ 的相对重要性。

**权重衰减**

这种特定的正则化选择在机器学习文献中称为权重衰减（weight decay），因为在顺序学习算法中，正则化项鼓励权重值衰减到零，除非数据支持它们。在统计学中，这是一种参数缩减方法，因为它将参数值缩小到零。其优势在于误差函数仍然是权重 $w$ 的二次函数，因此可以闭式找到其精确最小值。具体来说，将 (3.27) 对 $w$ 的梯度设为零，并如前所述求解 $w$，得到：

$$ w = (\lambda I + \Phi^T \Phi)^{-1} \Phi^T t $$

这代表了最小二乘解 (3.15) 的简单扩展。

**一般正则化器**

更一般的正则化器形式为：

$$ \frac{1}{2} \sum_{n=1}^N \{ t_n - w^T \phi(x_n) \}^2 + \frac{\lambda}{2} \sum_{j=1}^M |w_j|^q $$

其中 $q=2$ 对应于二次正则化器。对于 $q=1$，在统计学文献中称为套索（lasso）。如果 $\lambda$ 足够大，则某些系数 $w_j$ 会被驱动到零，导致模型稀疏化，即相应的基函数不起作用。

**最优值选择**

确定合适的正则化系数 $\lambda$ 是关键问题。选择过大或过小的 $\lambda$ 都会影响模型性能。通过交叉验证等方法可以有效选择最优值。

**图示说明**

图3.3展示了不同参数 $q$ 值下正则化函数的等高线，说明了不同正则化项对参数的影响。

**结论**

正则化最小二乘法通过在误差函数中引入正则化项，有效控制了过拟合现象，适用于有限大小的数据集。虽然正则化项的引入会增加计算复杂度，但其带来的模型性能提升和泛化能力的增强，使其在实际应用中具有重要意义。
