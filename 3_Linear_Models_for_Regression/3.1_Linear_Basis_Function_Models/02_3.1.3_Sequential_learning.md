# 02_3.1.3_Sequential_learning

"""
Lecture: 3_Linear_Models_for_Regression/3.1_Linear_Basis_Function_Models
Content: 02_3.1.3_Sequential_learning
"""

### TASK 1: 探索PDF

已经完成对PDF的探索。

### TASK 2: 深入详细分析

**3.1.3 Sequential Learning**

#### 概述
顺序学习算法，尤其是在处理大型数据集时，是一种极为重要的方法。与批量处理方法不同，顺序学习算法能够逐个或小批量地处理数据点，并在每次呈现后更新模型参数。这样的算法在实时应用中尤为重要，因为数据观测是在连续流中到达的，需要在看到所有数据点之前进行预测。

#### 详细分析

**顺序梯度下降法**

顺序学习算法的一个常见实现是随机梯度下降法（SGD）。SGD的更新规则如下：

$$ w(\tau+1) = w(\tau) - \eta \nabla E_n $$

其中，$\tau$ 表示迭代次数，$\eta$ 是学习率参数，$\nabla E_n$ 是对第 $n$ 个数据点的误差函数 $E_n$ 的梯度。该算法从初始向量 $w(0)$ 开始，对于平方和误差函数，更新规则为：

$$ w(\tau+1) = w(\tau) + \eta (t_n - w(\tau)^T \phi_n) \phi_n $$

这里，$\phi_n = \phi(x_n)$。该方法也称为最小均方算法（LMS）。

**学习率选择**

学习率 $\eta$ 的选择需要特别谨慎，以确保算法的收敛。过大的学习率可能导致算法发散，而过小的学习率则会导致收敛速度过慢。

**算法优势**

顺序学习算法的主要优势在于其适用于实时应用和大型数据集。在实时应用中，数据观测是连续流动的，因此必须在所有数据点到达之前进行预测。顺序算法只需一次一个地使用观测，并在使用完后丢弃它们，因此不需要存储整个数据集，适合处理大规模数据。

**最小均方算法（LMS）**

对于平方和误差函数，LMS 算法的更新公式为：

$$ w(\tau+1) = w(\tau) + \eta (t_n - w(\tau)^T \phi_n) \phi_n $$

这是一种简单而有效的算法，广泛应用于各种线性回归模型中。

#### 结论
顺序学习算法，特别是随机梯度下降法，是处理大型数据集和实时应用的有效工具。通过逐个更新模型参数，顺序算法不仅提高了计算效率，还减少了内存消耗，是现代机器学习中的重要方法。



### TASK 3: 构建Python代码

```python
import numpy as np

class SequentialLearning:
    """
    实现顺序学习算法类
    Attributes:
        learning_rate (float): 学习率
        weights (np.ndarray): 模型参数向量
    """

    def __init__(self, learning_rate: float, n_features: int):
        """
        初始化顺序学习算法类
        Args:
            learning_rate (float): 学习率
            n_features (int): 特征数量
        """
        self.learning_rate = learning_rate
        self.weights = np.zeros(n_features)

    def update_weights(self, x: np.ndarray, t: float) -> None:
        """
        更新模型参数
        Args:
            x (np.ndarray): 输入特征向量
            t (float): 目标值
        """
        prediction = np.dot(self.weights, x)
        error = t - prediction
        self.weights += self.learning_rate * error * x

    def train(self, X: np.ndarray, T: np.ndarray) -> None:
        """
        训练模型
        Args:
            X (np.ndarray): 输入特征矩阵
            T (np.ndarray): 目标值向量
        """
        for x, t in zip(X, T):
            self.update_weights(x, t)

    def predict(self, x: np.ndarray) -> float:
        """
        预测目标值
        Args:
            x (np.ndarray): 输入特征向量
        Returns:
            float: 预测值
        """
        return np.dot(self.weights, x)

    def evaluate(self, X: np.ndarray, T: np.ndarray) -> float:
        """
        评估模型性能
        Args:
            X (np.ndarray): 输入特征矩阵
            T (np.ndarray): 目标值向量
        Returns:
            float: 平均平方误差
        """
        predictions = np.dot(X, self.weights)
        errors = T - predictions
        return np.mean(errors ** 2)

if __name__ == "__main__":
    # 示例数据
    X_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])
    T_train = np.array([2.5, 3.5, 4.5, 5.5])

    model = SequentialLearning(learning_rate=0.01, n_features=2)
    model.train(X_train, T_train)

    X_test = np.array([[5, 6]])
    prediction = model.predict(X_test[0])
    print(f"Prediction: {prediction}")

    mse = model.evaluate(X_train, T_train)
    print(f"Mean Squared Error: {mse}")
```
