# 01_3.1.2_Geometry_of_least_squares

"""
Lecture: 3_Linear_Models_for_Regression/3.1_Linear_Basis_Function_Models
Content: 01_3.1.2_Geometry_of_least_squares
"""

### 详细分析 3.1.2 最小二乘法的几何解释

在本节中，我们详细讨论了最小二乘法（Least Squares Method）的几何解释。最小二乘法是回归分析中的一种常见方法，它通过最小化预测值与实际值之间的平方误差来找到模型参数。

#### 几何解释
首先，我们考虑一个N维空间，其坐标轴由目标变量$t_n$组成，因此$t = (t_1, t_2, ..., t_N)^T$是这个空间中的一个向量。每一个基函数$\phi_j(x_n)$在N个数据点处的值也可以表示为这个空间中的一个向量，记为$\phi_j$。这些向量与矩阵$\Phi$的列相对应。

当基函数的数量M小于数据点的数量N时，M个向量$\phi_j(x_n)$将会在N维空间中张成一个维度为M的线性子空间S。我们定义向量$y$为N维向量，其第n个元素为$y(x_n, w)$，其中$n = 1, ..., N$。由于$y$是向量$\phi_j$的线性组合，因此它可以在M维子空间S中的任意位置。

平方误差（Sum-of-Squares Error，$E(w)$）等于$y$与$t$之间的欧氏距离的平方。因此，最小二乘法解对应于位于子空间S中最接近$t$的$y$值。这实际上是$t$在子空间S上的正交投影，如图3.2所示。这可以通过以下方式验证：最小二乘解$y$由$\Phi w_{ML}$给出，这正是正交投影的形式。

#### 具体步骤
1. **定义目标向量和基函数向量**：
   - $t = (t_1, t_2, ..., t_N)^T$
   - $\phi_j = (\phi_j(x_1), \phi_j(x_2), ..., \phi_j(x_N))^T$
   
2. **构建子空间S**：由M个基函数向量$\phi_j$张成一个M维的线性子空间S。

3. **定义预测向量**：
   - $y = (y(x_1, w), y(x_2, w), ..., y(x_N, w))^T$
   
4. **最小化平方误差**：寻找位于子空间S中最接近目标向量$t$的预测向量$y$，即找到$t$在子空间S上的正交投影。

#### 例子
假设有一个简单的线性回归模型，其目标是通过最小二乘法找到模型参数，使得预测值与实际值之间的平方误差最小。通过将数据点表示为向量，并在高维空间中进行几何分析，我们可以清晰地理解最小二乘法的作用以及其几何意义。

通过这种几何解释，我们可以更直观地理解最小二乘法的本质，这对于深入理解回归分析中的各种方法具有重要意义。

 

### 代码实现

```python
import numpy as np
from scipy.linalg import svd

class LinearRegression:
    """
    使用最小二乘法进行线性回归的实现
    
    Attributes:
        weights (np.ndarray): 线性回归模型的权重
    """
    
    def __init__(self):
        self.weights = None
    
    def fit(self, X: np.ndarray, y: np.ndarray) -> None:
        """
        拟合线性回归模型
        
        Args:
            X (np.ndarray): 训练数据集的特征矩阵
            y (np.ndarray): 训练数据集的目标变量向量
        """
        X_bias = np.hstack([np.ones((X.shape[0], 1)), X])  # 添加偏置项
        self.weights = np.linalg.pinv(X_bias.T @ X_bias) @ X_bias.T @ y  # 使用伪逆计算权重
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        使用拟合好的模型进行预测
        
        Args:
            X (np.ndarray): 测试数据集的特征矩阵
            
        Returns:
            np.ndarray: 预测的结果
        """
        X_bias = np.hstack([np.ones((X.shape[0], 1)), X])  # 添加偏置项
        return X_bias @ self.weights

    def score(self, X: np.ndarray, y: np.ndarray) -> float:
        """
        计算模型的决定系数 R^2
        
        Args:
            X (np.ndarray): 测试数据集的特征矩阵
            y (np.ndarray): 测试数据集的目标变量向量
            
        Returns:
            float: 模型的 R^2 值
        """
        y_pred = self.predict(X)
        ss_total = np.sum((y - np.mean(y)) ** 2)
        ss_residual = np.sum((y - y_pred) ** 2)
        return 1 - (ss_residual / ss_total)

# 示例使用
if __name__ == "__main__":
    # 生成示例数据
    np.random.seed(0)
    X = 2 * np.random.rand(100, 1)
    y = 4 + 3 * X + np.random.randn(100, 1)
    
    # 拟合模型
    model = LinearRegression()
    model.fit(X, y)
    
    # 打印权重
    print("模型权重:", model.weights)
    
    # 预测
    X_new = np.array([[0], [2]])
    y_pred = model.predict(X_new)
    print("预测结果:", y_pred)
    
    # 计算 R^2 值
    r2_score = model.score(X, y)
    print("模型的 R^2 值:", r2_score)
```

#### 代码解释
- `LinearRegression`类实现了一个简单的线性回归模型，包含`fit`、`predict`和`score`方法。
- `fit`方法使用最小二乘法拟合模型，通过求解$w = (X^TX)^{-1}X^Ty$来计算权重。
- `predict`方法根据输入特征矩阵进行预测。
- `score`方法计算模型的决定系数$R^2$，用于评估模型的拟合优度。

这种实现方式确保了代码的可读性和可维护性，同时符合工业界的高可扩展性要求。通过添加中文注释，确保了代码的可读性和理解性。

**检查步骤**
- 确认输入数据的维度和格式。
- 验证权重的计算是否正确。
- 测试预测结果的准确性。
- 计算决定系数$R^2$验证模型的拟合优度。