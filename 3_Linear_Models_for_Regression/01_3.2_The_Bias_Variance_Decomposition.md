# 01_3.2_The_Bias-Variance_Decomposition

"""
Lecture: /3_Linear_Models_for_Regression
Content: 01_3.2_The_Bias-Variance_Decomposition
"""

## 3.2 偏差-方差分解

### 概述
在回归模型中，偏差（Bias）和方差（Variance）是影响模型性能的两个重要因素。偏差-方差分解（Bias-Variance Decomposition）是统计学习中的一个重要概念，它帮助我们理解和量化模型误差的来源。

### 定义
考虑一个输入输出对 $(x, t)$，其中 $t$ 是目标变量，$y(x; D)$ 是模型在数据集 $D$ 上的预测函数。理想的回归函数为 $h(x) = E[t|x]$。对于某个给定的数据集 $D$，模型的预测误差可以分解为偏差、方差和噪声三部分。

### 数学推导
我们从平方误差开始推导：

$$ E[L] = \int \{y(x; D) - h(x)\}^2 p(x) dx + \int \{h(x) - t\}^2 p(x, t) dx dt $$

其中，第一项是模型的误差，第二项是数据本身的噪声。

将 $\{y(x; D) - h(x)\}^2$ 展开并取期望：

$$ E_D[\{y(x; D) - h(x)\}^2] = E_D[\{y(x; D) - E_D[y(x; D)] + E_D[y(x; D)] - h(x)\}^2] $$

$$ = E_D[\{y(x; D) - E_D[y(x; D)]\}^2] + \{E_D[y(x; D)] - h(x)\}^2 $$

所以，期望平方误差可以分解为：

$$ E_D[\{y(x; D) - h(x)\}^2] = \{E_D[y(x; D)] - h(x)\}^2 + E_D[\{y(x; D) - E_D[y(x; D)]\}^2] $$

其中：
- $\{E_D[y(x; D)] - h(x)\}^2$ 是偏差的平方，表示模型预测的期望值与真实值之间的差距。
- $E_D[\{y(x; D) - E_D[y(x; D)]\}^2]$ 是方差，表示模型预测值在不同数据集上的变异程度。

### 综合误差
将单个输入 $x$ 的误差分解推广到整个输入空间 $X$，期望误差可以表示为：

$$ \text{expected loss} = (\text{bias})^2 + \text{variance} + \text{noise} $$

其中：
- 偏差的平方：$$ (\text{bias})^2 = \int \{E_D[y(x; D)] - h(x)\}^2 p(x) dx $$
- 方差：$$ \text{variance} = \int E_D[\{y(x; D) - E_D[y(x; D)]\}^2] p(x) dx $$
- 噪声：$$ \text{noise} = \int \{h(x) - t\}^2 p(x, t) dx dt $$

### 偏差-方差权衡
在实际应用中，模型的偏差和方差往往呈现反向变化关系：
- 复杂模型（低偏差，高方差）：能够很好地拟合训练数据，但在新数据上的泛化能力较差，容易过拟合。
- 简单模型（高偏差，低方差）：拟合能力有限，但在新数据上的泛化能力较好，不易过拟合。

最佳模型是偏差和方差之间的平衡，即模型在训练数据和新数据上都有良好的表现。

### 实例
考虑一个由正弦函数生成的数据集，每个数据集包含 $N = 25$ 个数据点，模型包含 24 个高斯基函数。在不同的正则化参数 $\lambda$ 下，模型的拟合结果和相应的平均拟合结果如下图所示：

![图3.5 不同 $\lambda$ 下模型拟合结果](#)

其中，左列显示了不同 $\lambda$ 值下模型对数据集的拟合结果，右列显示了 100 次拟合结果的平均值与生成数据的正弦函数的对比。

![图3.6 偏差和方差随 $\lambda$ 的变化](#)

如图3.6所示，偏差平方和方差的和以及平均测试集误差随 $\lambda$ 的变化情况。最小值出现在 $\ln \lambda = -0.31$ 附近，这与测试集误差的最小值接近。

### Python代码实现
下面是一个用Python实现偏差-方差分解的代码示例：

```python
import numpy as np

def bias_variance_decomposition(y_true: np.ndarray, y_pred: np.ndarray) -> tuple:
    """
    计算偏差和方差

    参数:
        y_true (np.ndarray): 真实值
        y_pred (np.ndarray): 预测值

    返回:
        tuple: 偏差平方和方差
    """
    # 计算期望预测值
    y_pred_mean = np.mean(y_pred, axis=0)
    
    # 计算偏差平方
    bias_squared = np.mean((y_pred_mean - y_true) ** 2)
    
    # 计算方差
    variance = np.mean(np.var(y_pred, axis=0))
    
    return bias_squared, variance

# 示例数据
y_true = np.sin(2 * np.pi * np.array([0.1, 0.4, 0.7, 1.0]))
y_pred = np.array([
    [0.9, 1.8, 2.7, 3.5],
    [1.0, 2.0, 3.0, 4.0],
    [1.1, 2.1, 3.1, 4.2]
])

bias_squared, variance = bias_variance_decomposition(y_true, y_pred)
print(f"偏差平方: {bias_squared}")
print(f"方差: {variance}")
```

### 代码解释
1. **函数定义**:
    - `bias_variance_decomposition` 函数计算给定真实值和预测值的偏差平方和方差。
2. **计算期望预测值**:
    - 使用 `np.mean` 计算预测值的平均值。
3. **计算偏差平方**:
    - 计算期望预测值与真实值之间的平方差。
4. **计算方差**:
    - 计算预测值的方差。
5. **示例数据**:
    - 给出真实值和预测值示例数据，并调用函数进行计算。

### 逻辑检查
- 使用 `np.mean` 和 `np.var` 函数确保计算的准确性和高效性。
- 通过打印重要信息（如偏差平方和方差）来验证函数的正确性。

这个代码实现了一个偏差-方差分解的示例，适用于分析和理解模型误差的来源和影响。

### 结论
通过偏差-方差分解，我们可以更好地理解和分析模型的性能。在实际应用中，选择合适的模型复杂度和正则化参数，可以有效地在偏差和方差之间取得平衡，从而构建出具有良好泛化能力的模型。

